{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why noy use linear regression\n",
    "\n",
    "# Logistic hypothesis\n",
    "\n",
    "# Parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTAÇÕES NECESSÁRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRIÇÃO DO PROBLEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que temos uma relação de pacientes com câncer de mama e precisamos desenvolver um modelo de Aprendizado de Máquina para prever qual a gravidade do câncer destes pacientes. Entre as respostas possíveis, temos que classificar o câncer como **maligno** ou **benigno**.\n",
    "\n",
    "Dessa forma, o problema não se encaixa como um problema de regressão, mas sim de classificação. Ou seja, queremos prever uma variável qualitativa, como uma categoria (**maligno** ou **benigno**).\n",
    "\n",
    "Para que os computadores possam \"entender\" as variáveis qualitativas, é preciso traduzi-las para a linguagem numérica, ex.: benigno = 0; e maligno = 1. Este processo de tradução é chamado de ***encoding*** ou **codificação**.\n",
    "\n",
    "Veremos mais à frente que estes valores qualitativos podem ser interpretados como a probabilidade de uma determinada amostra pertencer a classe-alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARREGAMENTO E VISUALIZAÇÃO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer(as_frame=True)\n",
    "\n",
    "print(f\"{'-'*80}\\nVariáveis independentes: {data['feature_names']} \\n{'-'*80}\\nVariáveis dependentes: {data['target_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"-\"*100} \\nX:')\n",
    "display(data['data'].head(5))\n",
    "\n",
    "print(f'{\"-\"*100} \\ny:')\n",
    "display(data['target'][-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisar a base de dados, observa-se que o valor correspondente à classe **maligno** é 0 e aquele correspondente à classe **benigno** é 1. É comum em um problema de classificação binária (duas classes-alvo) que a classe principal a qual se deseja prever seja representada pelo número 1. Dessa forma, por questões didáticas, decidi trocar os valores entre as classes. Ou seja, agora o valor referente à classe **maligno** (classe principal) será 1 e o da classe **benigno** será 0.\n",
    "\n",
    "**obs: essa mudança de ordem não altera em nada o processo de desenvolvimento e otimização do modelo de regressão logística.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['data'].values, data['target'].values\n",
    "\n",
    "y = np.where(y == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.scatter(X[:,2][y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='benigno')\n",
    "ax0.scatter(X[:,2][y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='maligno')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POR QUE NÃO USAR REGRESSÃO LINEAR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez transformadas as classes em valores binários 0/1, podemos treinar um regressor linear para estimar qual a categoria do câncer de um determinado paciente da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.linspace(X[:,2].min(), X[:,2].max(), 10)\n",
    "y_ = x_ * (1 / 40) - 2 # Linear Regressor\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.scatter(X[:,2][y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(X[:,2][y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0.plot(x_, y_, linestyle='--', c='black', label='Regressor Linear')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando a figura anterior, podemos interpretar os valores preditos pelo regressor linear como uma estimativa de $P(y=1|X)$. Isto é, a probabilidade de uma determinada amostra $X$ pertencer a classe 1 (maligno). Observa-se que os valores estimados pelo regressor linear ultrapassam os limites de probabilidae (0 e 1) e podem variar de $-\\infty$ a $+\\infty$.\n",
    "\n",
    "Portanto, é por essas e outras razões que é preferível a utilização de um regressor mais adequado capaz de lidar com valores qualitativos, como o modelo de **Regressão Logística**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESSÃO LOGÍSTICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo de Regressão Logística tem esse nome devido à função que serve de hipótese para o modelo: função logística, mais especificamente a função sigmóide.\n",
    "\n",
    "A imagem da função sigmóide varia entre os valores 0 e 1. Sua equação é descrita da seguinte forma:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Veremos mais à frente que, quando utilizada para modelar um problema de classificação do tipo $P(y=1|X)$, a função precisa ser ajustada e $z$ passa a ser uma abreviação de $w \\cdot X + b$ e a versão completa da função é, então:\n",
    "\n",
    "$$ g_{W, b}(X) = \\frac{1}{1 + e^{-(W \\cdot X + b)}} $$\n",
    "\n",
    "onde $g_{W, b}(X) = P(y=1|X)$ é a variável dependente (saída), $X$ são as variáveis independentes (entrada), $W$ e $b$ são parâmetros a serem otimizados.\n",
    "\n",
    "**OBS: para facilitar, a partir de agora, vamos abreviar $g_{W, b}(X)$ por $\\hat{y}$, onde entende-se que $g(.)$ é a resposta calculada pelo modelo ($g(.)=\\hat{y}$)**\n",
    "\n",
    "A forma em \"S\" da função sigmoide pode ser visualizada na figura a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10, 10, 100)\n",
    "g = sigmoid(z)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.plot(z, g, c='black', label='Sigmoid')\n",
    "ax0.set_xlabel('$z$')\n",
    "ax0.set_ylabel('$g(z)$')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos tentar implementar a função logística para receber como entrada as variáveis do nosso problema de classificação de câncer de mama e ver como os parâmetros influenciam a função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.linspace(-50, X[:,2].max(), 1000)\n",
    "\n",
    "w0, b0 = 1, 0\n",
    "w1, b1 = 1, -5\n",
    "w2, b2 = 0.05, 0\n",
    "w3, b3 = 0.05, -5\n",
    "\n",
    "z0 = x_ * w0 + b0\n",
    "z1 = x_ * w1 + b1\n",
    "z2 = x_ * w2 + b2\n",
    "z3 = x_ * w3 + b3\n",
    "\n",
    "g0 = sigmoid(z0)\n",
    "g1 = sigmoid(z1)\n",
    "g2 = sigmoid(z2)\n",
    "g3 = sigmoid(z3)\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.scatter(X[:,2][y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(X[:,2][y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0.plot(x_, g0, linestyle='--', c='black', label=f'w={w0} | b={b0}')\n",
    "ax0.plot(x_, g1, linestyle='--', c='orange', label=f'w={w1} | b={b1}')\n",
    "ax0.plot(x_, g2, linestyle='--', c='purple', label=f'w={w2} | b={b2}')\n",
    "ax0.plot(x_, g3, linestyle='--', c='blue', label=f'w={w3} | b={b3}')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTIMIZAÇÃO DOS PARÂMETROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando desejamos modelar um regressor logístico para um problema de classificação, estimamos uma função logística cujos coeficientes $W$ e $b$ são desconhecidos. Portanto, é necessário estimar os coeficientes de maneira que a curva melhor se ajuste aos dados de treinamento.\n",
    "\n",
    "Para a otimização dos coeficientes do modelo de Regressão Linear, é utilizado o método dos Mínimos Quadrados para minimizar a diferença entre os dados preditos $\\hat{y}$ e reais $y$ da seguinte forma:\n",
    "\n",
    "$$ MSE = \\frac{1}{2m} \\sum_{i=0}^{m-1} (\\hat{y_{i}} - y_{i})^{2} $$\n",
    "\n",
    "A utilização da MSE como função custo da Regressão Logística não é indicada e abaixo demonstraremos o porquê:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y):\n",
    "    return np.mean((y_pred - y)**2, axis=0) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizar utils\n",
    "\n",
    "def standardize(x):\n",
    "    return (x - x.mean(axis=0)) / x.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 25\n",
    "\n",
    "w_ = np.linspace(-25, 25, n_grid).reshape(1,-1)\n",
    "b_ = np.linspace(-25, 25, n_grid).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "x_ = standardize(X[:, 2].reshape(-1,1))\n",
    "z_ = np.dot(x_, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "y_pred = sigmoid(z_)\n",
    "\n",
    "mse_ = mse(y_pred, np.tile(y.reshape(-1,1), n_grid**2))\n",
    "mse_ = mse_.reshape(n_grid, n_grid)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "ax0 = fig.add_subplot(111, projection='3d')\n",
    "ax0.plot_surface(W_, B_, mse_, cmap='viridis')\n",
    "ax0.set_xlabel('$W$')\n",
    "ax0.set_ylabel('$b$')\n",
    "ax0.set_zlabel('$MSE$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função MSE, quando utilizada para a Regressão Logística, assume uma forma não-convexa. Isto é, existem vários pontos da função custo diferentes do mínimo global que também possuem derivadas parciais com relação aos parâmetros iguais a 0. Dessa forma, a otimização dos parâmetros através do metódo Gradiente Descendente é prejudicada, uma vez que quando o gradiente é nulo não há atualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10, 10, 100)\n",
    "g = sigmoid(z)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.plot(z, g, c='black', label='Sigmoid')\n",
    "ax0.set_xlabel('$z$')\n",
    "ax0.set_ylabel('$g(z)$')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para contornar esse problema, na Regressão Logística é utilizada a função logarítimica para estimar o erro de um determinado modelo, da seguinte forma:\n",
    "\n",
    "$$ J(y, \\hat{y}) = \\begin{equation*}\n",
    "        \\begin{cases}\n",
    "            \\frac{1}{m} \\sum_{i=0}^{m-1} -log(\\hat{y}_{i}) \\quad\\quad\\quad \\text{   se } y_{i} = 1 \\\\ \\\\\n",
    "            \\frac{1}{m} \\sum_{i=0}^{m-1} -log(1 - \\hat{y}_{i}) \\quad\\space \\text{   se } y_{i} = 0\n",
    "        \\end{cases}\n",
    "    \\end{equation*} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_(y, y_pred, c=0):\n",
    "    if c==0:\n",
    "        return -np.log(1 - y_pred[y==c]).mean(axis=0)\n",
    "    elif c==1:\n",
    "        return -np.log(y_pred[y==c]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = np.ones(n_grid).reshape(1,-1) * 15\n",
    "\n",
    "z_ = np.dot(x_, w_) + b_\n",
    "y_pred = sigmoid(z_)\n",
    "\n",
    "loss0 = log_loss_(y, y_pred, 0)\n",
    "loss1 = log_loss_(y, y_pred, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(111)\n",
    "ax0.plot(b_.flatten(), loss0, label='$y=0$')\n",
    "ax0.plot(b_.flatten(), loss1, label='$y=1$')\n",
    "ax0.set_xlabel('$b$')\n",
    "ax0.set_ylabel('$L(y, \\hat{y})$')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos juntar as duas expressões em uma só, da seguinte forma:\n",
    "\n",
    "$$ J(y, \\hat{y}) = - \\frac{1}{m} \\sum_{i=0}^{m-1} (y) \\cdot log(\\hat{y_{i}}) + (1 - y) \\cdot log(1 - \\hat{y_{i}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, y_pred):\n",
    "    return - np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 25\n",
    "\n",
    "w_ = np.linspace(-10, 10, n_grid).reshape(1,-1)\n",
    "b_ = np.linspace(-10, 10, n_grid).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "x_ = standardize(X[:, 2].reshape(-1,1))\n",
    "z_ = np.dot(x_, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "y_pred = sigmoid(z_)\n",
    "\n",
    "loss_ = log_loss(np.tile(y.reshape(-1,1), n_grid**2), y_pred)\n",
    "loss_ = loss_.reshape(n_grid, n_grid)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "ax0 = fig.add_subplot(111, projection='3d')\n",
    "ax0.plot_surface(W_, B_, loss_, cmap='viridis')\n",
    "ax0.set_xlabel('$W$')\n",
    "ax0.set_ylabel('$b$')\n",
    "ax0.set_zlabel('$J(y, \\hat{y})$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de perda logarítimica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
