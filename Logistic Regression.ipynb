{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTAÇÕES NECESSÁRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from ipywidgets import interact\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRIÇÃO DO PROBLEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que temos uma relação de pacientes com câncer de mama e precisamos desenvolver um modelo de Aprendizado de Máquina para prever qual a gravidade do câncer destes pacientes. Entre as respostas possíveis, temos que classificar o câncer como **maligno** ou **benigno**.\n",
    "\n",
    "Dessa forma, o problema não se encaixa como um problema de regressão, mas sim de classificação. Ou seja, queremos prever uma variável qualitativa, como uma categoria (**maligno** ou **benigno**).\n",
    "\n",
    "Para que os computadores possam \"entender\" as variáveis qualitativas, é preciso traduzi-las para a linguagem numérica, ex.: benigno = 0; e maligno = 1. Este processo de tradução é chamado de ***encoding*** ou **codificação**.\n",
    "\n",
    "Veremos mais à frente que estes valores qualitativos podem ser interpretados como a probabilidade de uma determinada amostra pertencer a classe-alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARREGAMENTO E VISUALIZAÇÃO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer(as_frame=True)\n",
    "\n",
    "print(f\"{'-'*80}\\nVariáveis independentes: {data['feature_names']} \\n{'-'*80}\\nVariáveis dependentes: {data['target_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"-\"*100} \\nX:')\n",
    "display(data['data'].head(5))\n",
    "\n",
    "print(f'{\"-\"*100} \\ny:')\n",
    "display(data['target'][-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisar a base de dados, observa-se que o valor correspondente à classe **maligno** é 0 e aquele correspondente à classe **benigno** é 1. É comum em um problema de classificação binária (duas classes-alvo) que a classe principal a qual se deseja prever seja representada pelo número 1. Dessa forma, por questões didáticas, decidi trocar os valores entre as classes. Ou seja, agora o valor referente à classe **maligno** (classe principal) será 1 e o da classe **benigno** será 0.\n",
    "\n",
    "**obs: essa mudança de ordem não altera em nada o processo de desenvolvimento e otimização do modelo de regressão logística.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['data'].values, data['target'].values\n",
    "\n",
    "y = np.where(y == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.scatter(X[:,2][y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='benigno')\n",
    "ax0.scatter(X[:,2][y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='maligno')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POR QUE NÃO USAR REGRESSÃO LINEAR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez transformadas as classes em valores binários 0/1, podemos treinar um regressor linear para estimar qual a categoria do câncer de um determinado paciente da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.linspace(X[:,2].min(), X[:,2].max(), 10)\n",
    "y_ = x_ * (1 / 40) - 2 # Linear Regressor\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.scatter(X[:,2][y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(X[:,2][y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0.plot(x_, y_, linestyle='--', c='black', label='Regressor Linear')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando a figura anterior, podemos interpretar os valores preditos pelo regressor linear como uma estimativa de $P(y=1|X)$. Isto é, a probabilidade de uma determinada amostra $X$ pertencer a classe 1 (maligno). Observa-se que os valores estimados pelo regressor linear ultrapassam os limites de probabilidae (0 e 1) e podem variar de $-\\infty$ a $+\\infty$.\n",
    "\n",
    "Portanto, é por essas e outras razões que é preferível a utilização de um regressor mais adequado capaz de lidar com valores qualitativos, como o modelo de **Regressão Logística**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESSÃO LOGÍSTICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo de Regressão Logística tem esse nome devido à função que serve de hipótese para o modelo: função logística, mais especificamente a função sigmóide.\n",
    "\n",
    "A imagem da função sigmóide varia entre os valores 0 e 1. Sua equação é descrita da seguinte forma:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Veremos mais à frente que, quando utilizada para modelar um problema de classificação do tipo $P(y=1|X)$, a função precisa ser ajustada e $z$ passa a ser uma abreviação de $w \\cdot X + b$ e a versão completa da função é, então:\n",
    "\n",
    "$$ g_{W, b}(X) = \\frac{1}{1 + e^{-(W \\cdot X + b)}} $$\n",
    "\n",
    "onde $g_{W, b}(X) = P(y=1|X)$ é a variável dependente (saída), $X$ são as variáveis independentes (entrada), $W$ e $b$ são parâmetros a serem otimizados.\n",
    "\n",
    "**OBS: para facilitar, a partir de agora, vamos abreviar $g_{W, b}(X)$ por $\\hat{y}$, onde entende-se que $g(.)$ é a resposta calculada pelo modelo ($g(.)=\\hat{y}$)**\n",
    "\n",
    "A forma em \"S\" da função sigmoide pode ser visualizada na figura a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10, 10, 100)\n",
    "g = sigmoid(z)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.plot(z, g, c='black', label='Sigmoid')\n",
    "ax0.set_xlabel('$z$')\n",
    "ax0.set_ylabel('$g(z)$')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos tentar implementar a função logística para receber como entrada as variáveis do nosso problema de classificação de câncer de mama e ver como os parâmetros influenciam a função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizar utils\n",
    "\n",
    "def standardize(x):\n",
    "    return (x - x.mean(axis=0)) / x.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.linspace(X[:,2].min(), X[:,2].max(), 1000)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "def update(w, b):\n",
    "\n",
    "    z = w * x_ + b\n",
    "    g = sigmoid(z)\n",
    "\n",
    "    ax0.clear()\n",
    "    ax0.scatter(X[:,2][y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "    ax0.scatter(X[:,2][y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "    ax0.plot(x_, g, linestyle='--', c='black', label=f'w={round(w, 2)} | b={round(b, 2)}')\n",
    "    ax0.set_xlabel(data['feature_names'][2])\n",
    "    ax0.set_ylabel('Class')\n",
    "    ax0.legend()\n",
    "\n",
    "interact(update, w=widgets.FloatSlider(value=0, min=-10, max=10), b=widgets.FloatSlider(value=0, min=-10, max=10))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTIMIZAÇÃO DOS PARÂMETROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando desejamos modelar um regressor logístico para um problema de classificação, estimamos uma função logística cujos coeficientes $W$ e $b$ são desconhecidos. Portanto, é necessário estimar os coeficientes de maneira que a curva melhor se ajuste aos dados de treinamento.\n",
    "\n",
    "Para a otimização dos coeficientes do modelo de Regressão Linear, é utilizado o método dos Mínimos Quadrados para minimizar a diferença entre os dados preditos $\\hat{y}$ e reais $y$ da seguinte forma:\n",
    "\n",
    "$$ MSE = \\frac{1}{2m} \\sum_{i=0}^{m-1} (\\hat{y_{i}} - y_{i})^{2} $$\n",
    "\n",
    "A utilização da MSE como função custo da Regressão Logística não é indicada e abaixo demonstraremos o porquê:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y):\n",
    "    return np.mean((y_pred - y)**2, axis=0) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 25\n",
    "\n",
    "w_ = np.linspace(-25, 25, n_grid).reshape(1,-1)\n",
    "b_ = np.linspace(-25, 25, n_grid).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "x_ = X[:, 2].reshape(-1,1)\n",
    "z_ = np.dot(x_, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "y_pred = sigmoid(z_)\n",
    "\n",
    "mse_ = mse(y_pred, np.tile(y.reshape(-1,1), n_grid**2))\n",
    "mse_ = mse_.reshape(n_grid, n_grid)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "ax0 = fig.add_subplot(111, projection='3d')\n",
    "ax0.plot_surface(W_, B_, mse_, cmap='viridis')\n",
    "ax0.set_xlabel('$W$')\n",
    "ax0.set_ylabel('$b$')\n",
    "ax0.set_zlabel('$MSE$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido ao fato de a função logística (sigmoide) ser não-linear, a função MSE, quando utilizada no contexto da Regressão Logística, assume uma forma não-convexa. Isto é, existem vários pontos da função custo diferentes do mínimo global que também possuem derivadas parciais com relação aos parâmetros iguais a 0. Dessa forma, a otimização dos parâmetros através do metódo Gradiente Descendente é prejudicada, uma vez que quando o gradiente é nulo não há atualização.\n",
    "\n",
    "Ao invés de utilizar a MSE, é utilizada uma abordagem diferente: da Probabilidade Máxima (*Maximum Likelihood*). A estimativa dos coeficientes de um modelo de Regressão Logística é dada de tal forma a maximizar as probabilidades de classificação em cada classe.\n",
    "\n",
    "$ \\begin{equation}\n",
    "    \\begin{align*}\n",
    "        \\text{Likelihood} = L(y, \\hat{y}) & = \\prod_{i=0}^{m-1} P(y_{i}=1|X_{i})^{y_{i}} \\prod_{i=0}^{m-1} P(y_{i}=0|X_{i})^{1-y_{i}} \\\\\n",
    "        & = \\prod_{i=0}^{m-1} P(y_{i}=1|X_{i})^{y_{i}} \\prod_{i=0}^{m-1} \\left[ 1 - P(y_{i}=1|X_{i}) \\right]^{1-y_{i}} \\\\\n",
    "        & = \\prod_{i=0}^{m-1} \\hat{y_{i}}^{y_{i}} \\prod_{i=0}^{m-1} \\left( 1 - \\hat{y_{i}} \\right)^{1-y_{i}}\n",
    "    \\end{align*}\n",
    "\\end{equation} $\n",
    "\n",
    "Através do produtório das probabilidades preditas para cada classe, a função *Likelihood* mede o nível de acerto do modelo em uma escala de probabilidade $[0, 1]$. Na prática, a probabilidade predita ($\\hat{y}$) nunca será igual a 0 ou 1, de fato, porque a função sigmoid nunca alcança esses valores. Entretanto, quanto mais perto as probabilidades preditas forem das verdadeiras, maior será o valor do *Likelihood*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(y, y_hat):\n",
    "    return np.prod(y_hat**y * (1 - y_hat)**(1-y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,2].reshape(-1,1)\n",
    "x_ = np.linspace(x.min(), x.max(), 1000)\n",
    "\n",
    "N = 25\n",
    "\n",
    "w_max = b_max = 10\n",
    "w_min = b_min = -w_max\n",
    "\n",
    "w_ = np.linspace(w_min, w_max, N).reshape(1,-1)\n",
    "b_ = np.linspace(b_min, b_max, N).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "Z_ = np.dot(x, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "Y_hat_ = sigmoid(Z_)\n",
    "\n",
    "L_ = likelihood(np.tile(y.reshape(-1,1), N**2), Y_hat_)\n",
    "L_ = L_.reshape(N, N)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax0.scatter(x[y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(x[y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0_line, = ax0.plot(x_, x_, linestyle='--', c='black')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.set_ylim(-.1, 1.1)\n",
    "ax0.legend()\n",
    "\n",
    "ax1.plot_surface(W_, B_, L_, cmap='viridis', zorder=0)\n",
    "ax1_line, = ax1.plot([], [], [], marker='o', color='orange', zorder=3)\n",
    "ax1.set_xlabel('$W$')\n",
    "ax1.set_ylabel('$b$')\n",
    "ax1.set_zlabel('Likelihood')\n",
    "\n",
    "def update(w, b):\n",
    "\n",
    "    z_ = w * x_ + b\n",
    "    z = w * x + b\n",
    "    y_hat_ = sigmoid(z_)\n",
    "    y_hat = sigmoid(z)\n",
    "    lh = likelihood(y, y_hat.flatten())\n",
    "    \n",
    "    ax0_line.set_ydata(y_hat_)\n",
    "    ax0_line.set_label(f'w:{w} | b:{b}')\n",
    "    ax0.set_title(f'Likelihood: {lh}')\n",
    "\n",
    "    ax1_line.set_data([w], [b])\n",
    "    ax1_line.set_3d_properties(lh)\n",
    "\n",
    "interact(update, w=widgets.FloatSlider(value=1, min=w_min, max=w_max), b=widgets.FloatSlider(value=0, min=b_min, max=b_max))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando os resultados da função *Likelihood*, tem-se que, devido ao produtório de uma série de valores menores que 1 e maiores que 0, o valor da função é, para a maioria dos coeficientes, próximo de zero e, portanto, métodos que dependem do gradiente para a atualização dos coeficientes não funcionarão.\n",
    "\n",
    "Para contornar esse problema, a função logarítmica é aplicada à função *Likelihood*: \n",
    "\n",
    "$$ \\begin{equation*}\n",
    "    log \\left(L(y, \\hat{y}) \\right) = log\\left(\\prod_{i=0}^{m-1} \\hat{y_{i}}^{y_{i}} \\prod_{i=0}^{m-1} \\left( 1 - \\hat{y_{i}} \\right)^{1-y_{i}}\\right)\n",
    "\\end{equation*} $$\n",
    "\n",
    "Aplicando a seguinte propriedade do logarítmo: $log(A \\cdot B)=log(A)+log(B)$ e chamando $log(L(y, \\hat{y}))$ de $LL(y, \\hat{y})$ temos que\n",
    "\n",
    "$$ \\begin{equation*}\n",
    "    \\begin{align*}\n",
    "        LL(y, \\hat{y}) & = \\sum_{i=0}^{m-1} log(\\hat{y_{i}}^{y_{i}}) + log([1 - \\hat{y_{i}}]^{1 - y_{i}}) \\\\\n",
    "        & = \\sum_{i=0}^{m-1} y_{i} \\cdot log(\\hat{y_{i}}) + (1 - y_{i}) log([1 - \\hat{y_{i}}]) \\\\\n",
    "    \\end{align*}\n",
    "\\end{equation*} $$\n",
    "\n",
    "Com isso, temos a expressão para o *log-likelihood*. Sua forma pode ser visualizada na figura abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(y, y_hat):\n",
    "    return np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,2].reshape(-1,1)\n",
    "x_ = np.linspace(x.min(), x.max(), 1000)\n",
    "\n",
    "N = 25\n",
    "\n",
    "w_max = b_max = 10\n",
    "w_min = b_min = -w_max\n",
    "\n",
    "w_ = np.linspace(w_min, w_max, N).reshape(1,-1)\n",
    "b_ = np.linspace(b_min, b_max, N).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "Z_ = np.dot(x, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "Y_hat_ = sigmoid(Z_)\n",
    "\n",
    "L_ = log_likelihood(np.tile(y.reshape(-1,1), N**2), Y_hat_)\n",
    "L_ = L_.reshape(N, N)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax0.scatter(x[y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(x[y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0_line, = ax0.plot(x_, x_, linestyle='--', c='black')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.set_ylim(-.1, 1.1)\n",
    "ax0.legend()\n",
    "\n",
    "ax1.plot_surface(W_, B_, L_, cmap='viridis', zorder=0)\n",
    "ax1_line, = ax1.plot([], [], [], marker='o', color='orange', zorder=3)\n",
    "ax1.set_xlabel('$W$')\n",
    "ax1.set_ylabel('$b$')\n",
    "ax1.set_zlabel('Likelihood')\n",
    "\n",
    "def update(w, b):\n",
    "\n",
    "    z_ = w * x_ + b\n",
    "    z = w * x + b\n",
    "    y_hat_ = sigmoid(z_)\n",
    "    y_hat = sigmoid(z)\n",
    "    lh = log_likelihood(y, y_hat.flatten())\n",
    "    \n",
    "    ax0_line.set_ydata(y_hat_)\n",
    "    ax0_line.set_label(f'w:{w} | b:{b}')\n",
    "    ax0.set_title(f'Likelihood: {lh}')\n",
    "\n",
    "    ax1_line.set_data([w], [b])\n",
    "    ax1_line.set_3d_properties(lh)\n",
    "\n",
    "interact(update, w=widgets.FloatSlider(value=1, min=w_min, max=w_max), b=widgets.FloatSlider(value=0, min=b_min, max=b_max))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a otimização dos coeficientes da Regressão linear é comum utilizar métodos de minimização de funções, em detrimento daqueles de maximização. Portanto, na realidade, a função cuja **minimização** significa encontrar os melhores coeficientes (função custo) é o negativo do *log-likelihood*:\n",
    "\n",
    "$$ -L(y, \\hat{y}) = J(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m-1} -y_{i} \\cdot log(\\hat{y_{i}}) -(1 - y_{i}) \\cdot log(1 - \\hat{y_{i}}) $$\n",
    "\n",
    "A forma de $J(y, \\hat{y})$ pode ser visualizada na figura abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,2].reshape(-1,1)\n",
    "x_ = np.linspace(x.min(), x.max(), 1000)\n",
    "\n",
    "N = 25\n",
    "\n",
    "w_max = b_max = 10\n",
    "w_min = b_min = -w_max\n",
    "\n",
    "w_ = np.linspace(w_min, w_max, N).reshape(1,-1)\n",
    "b_ = np.linspace(b_min, b_max, N).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "Z_ = np.dot(x, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "Y_hat_ = sigmoid(Z_)\n",
    "\n",
    "L_ = log_loss(np.tile(y.reshape(-1,1), N**2), Y_hat_)\n",
    "L_ = L_.reshape(N, N)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax0.scatter(x[y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(x[y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0_line, = ax0.plot(x_, x_, linestyle='--', c='black')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.set_ylim(-.1, 1.1)\n",
    "ax0.legend()\n",
    "\n",
    "ax1.plot_surface(W_, B_, L_, cmap='viridis', zorder=0)\n",
    "ax1_line, = ax1.plot([], [], [], marker='o', color='orange', zorder=3)\n",
    "ax1.set_xlabel('$W$')\n",
    "ax1.set_ylabel('$b$')\n",
    "ax1.set_zlabel('Likelihood')\n",
    "\n",
    "def update(w, b):\n",
    "\n",
    "    z_ = w * x_ + b\n",
    "    z = w * x + b\n",
    "    y_hat_ = sigmoid(z_)\n",
    "    y_hat = sigmoid(z)\n",
    "    lh = log_loss(y, y_hat.flatten())\n",
    "    \n",
    "    ax0_line.set_ydata(y_hat_)\n",
    "    ax0_line.set_label(f'w:{w} | b:{b}')\n",
    "    ax0.set_title(f'Likelihood: {lh}')\n",
    "\n",
    "    ax1_line.set_data([w], [b])\n",
    "    ax1_line.set_3d_properties(lh)\n",
    "\n",
    "interact(update, w=widgets.FloatSlider(value=1, min=w_min, max=w_max), b=widgets.FloatSlider(value=0, min=b_min, max=b_max))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a obtenção da função custo para a Regressão Logística, podemos observar que a função resultante é agora convexa e suave. Além de os valores estarem adequados, nem muito pequenos, nem muito grandes.\n",
    "\n",
    "Encontrar os parâmetros que minimizem essa função significa encontrar aqueles que satisfaçam a equação:\n",
    "\n",
    "$$ \\nabla_{w,b} J(y, \\hat{y}) = \\frac{\\partial J}{\\partial w}\\textbf{i} + \\frac{\\partial J}{\\partial b}\\textbf{j} = 0 \\Rightarrow \\begin{cases}\n",
    "                \\frac{\\partial J}{\\partial w} = 0 \\\\ \\\\\n",
    "                \\frac{\\partial J}{\\partial b} = 0\n",
    "            \\end{cases} $$\n",
    "\n",
    "Para isso, então, é necessário calcular as derivadas parciais da função custo com relação aos coeficientes $w$ e $b$.\n",
    "\n",
    "Primeiro, vamos calcular a derivada com relação a $w$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial w} & = \\frac{\\partial}{\\partial w} \\left[\\frac{1}{m} \\sum_{i=0}^{m-1} - y_{i} \\cdot log(\\hat{y_{i}}) - (1 - y_{i}) \\cdot log(1 - \\hat{y_{i}}) \\right] \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} - y_{i} \\cdot \\frac{\\partial}{\\partial w} log(\\hat{y_{i}}) - (1 - y_{i}) \\cdot \\frac{\\partial}{\\partial w} log(1 - \\hat{y_{i}})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Calculando $\\frac{\\partial}{\\partial w} log(\\hat{y_{i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial w} log(\\hat{y_{i}}) = \\frac{\\partial}{\\partial w} log\\left(\\frac{1}{1 + e^{wx+b}}\\right) $$\n",
    "\n",
    "Para isso, vamos fazer as seguintes substituições de variáveis:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    g = log(\\frac{1}{1 + e^{wx + b}}) = log(\\hat{y}) \\\\\n",
    "    k = \\frac{1}{1 + e^{wx + b}} = \\hat{y} \\\\\n",
    "    v = 1 + e^{wx + b} = \\hat{y}^{-1} \\\\\n",
    "    u = wx + b\n",
    "\\end{cases} $$\n",
    "\n",
    "Dessa forma, de acordo com a regra da cadeia, temos que $\\frac{\\partial g}{\\partial w} = \\frac{\\partial g}{\\partial k} \\cdot \\frac{\\partial k}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u} \\cdot \\frac{\\partial u}{\\partial w}$.\n",
    "\n",
    "Resolvendo as derivadas parciais, temos\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    \\frac{\\partial g}{\\partial k} = \\frac{1}{k} = \\hat{y}^{-1} \\\\\n",
    "    \\frac{\\partial k}{\\partial v} = -v^{-2} = -\\hat{y}^{2} \\\\\n",
    "    \\frac{\\partial v}{\\partial u} = e^{u} = \\hat{y}^{-1} - 1 \\\\\n",
    "    \\frac{\\partial u}{\\partial w} = x\n",
    "\\end{cases} $$\n",
    "\n",
    "Substituindo, temos\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    \\frac{\\partial g}{\\partial w} & = \\frac{\\partial g}{\\partial k} \\cdot \\frac{\\partial k}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u} \\cdot \\frac{\\partial u}{\\partial w} \\\\\n",
    "    & = \\hat{y}^{-1} \\cdot (-\\hat{y}^{2}) \\cdot (\\hat{y}^{-1} - 1) \\cdot x \\\\\n",
    "    & = -\\hat{y} \\cdot (\\hat{y}^{-1} - 1) \\cdot x \\\\\n",
    "    & = (\\hat{y} - 1) \\cdot x\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Calculando $\\frac{\\partial}{\\partial w} log(1 - \\hat{y_{i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial w} log( 1 - \\hat{y_{i}}) = \\frac{\\partial}{\\partial w} log\\left(1 - \\frac{1}{1 + e^{wx+b}}\\right) $$\n",
    "\n",
    "Para isso, vamos fazer as seguintes substituições de variáveis:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    g = log\\left(1 - \\frac{1}{1 + e^{wx+b}}\\right) = log(1 - \\hat{y}) \\\\\n",
    "    k = 1 - \\frac{1}{1 + e^{wx + b}} = 1 - \\hat{y} \\\\\n",
    "    v = 1 + e^{wx + b} = \\hat{y}^{-1} \\\\\n",
    "    u = wx + b\n",
    "\\end{cases} $$\n",
    "\n",
    "Dessa forma, de acordo com a regra da cadeia, temos que $\\frac{\\partial g}{\\partial w} = \\frac{\\partial g}{\\partial k} \\cdot \\frac{\\partial k}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u} \\cdot \\frac{\\partial u}{\\partial w}$.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolvendo as derivadas parciais, temos\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    \\frac{\\partial g}{\\partial k} = \\frac{1}{k} = \\frac{1}{1 - \\hat{y}} \\\\\n",
    "    \\frac{\\partial k}{\\partial v} = v^{-2} = \\hat{y}^{2} \\\\\n",
    "    \\frac{\\partial v}{\\partial u} = e^{u} = \\hat{y}^{-1} - 1 \\\\\n",
    "    \\frac{\\partial u}{\\partial w} = x\n",
    "\\end{cases} $$\n",
    "\n",
    "Substituindo, temos\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    \\frac{\\partial g}{\\partial w} & = \\frac{\\partial g}{\\partial k} \\cdot \\frac{\\partial k}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u} \\cdot \\frac{\\partial u}{\\partial w} \\\\\n",
    "    & = \\frac{1}{1 - \\hat{y}} \\cdot \\hat{y}^{2} \\cdot (\\hat{y}^{-1} - 1) \\cdot x \\\\\n",
    "    & = \\frac{\\hat{y} \\cdot (1 - \\hat{y})}{1 - \\hat{y}} \\cdot x \\\\\n",
    "    & = \\hat{y} \\cdot x\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituindo os resultados de $\\frac{\\partial}{\\partial w} log(\\hat{y_{i}})$ e $\\frac{\\partial}{\\partial w} log(1 - \\hat{y_{i}})$ em $\\frac{\\partial J}{\\partial w}$, temos\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial w} & = \\frac{1}{m} \\sum_{i=0}^{m-1} - y_{i} \\cdot \\frac{\\partial}{\\partial w} log(\\hat{y_{i}}) - (1 - y_{i}) \\cdot \\frac{\\partial}{\\partial w} log(1 - \\hat{y_{i}}) \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} - y_{i} \\cdot (\\hat{y}_{i} - 1) \\cdot x_{i} - (1 - y_{i}) \\cdot \\hat{y}_{i} \\cdot x_{i} \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} [- y_{i} \\cdot (\\hat{y}_{i} - 1) - (1 - y_{i}) \\cdot \\hat{y}_{i}] \\cdot x_{i} \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} (-y_{i} \\hat{y_{i}} + y_{i} - \\hat{y}_{i} + y_{i} \\hat{y}_{i}) \\cdot x_{i} \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} (y_{i} - \\hat{y}_{i}) \\cdot x_{i}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo os mesmos passos anteriores para $\\frac{\\partial J}{\\partial b}$ temos ao final:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    \\frac{\\partial J}{\\partial w} = (y_{i} - \\hat{y}_{i}) \\cdot x_{i} \\\\ \\\\\n",
    "    \\frac{\\partial J}{\\partial b} = (y_{i} - \\hat{y}_{i})\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As equações das derivadas parciais obtidas com relação ao valor predito $\\hat{y}$ são idênticas às da Regressão Linear. Entretanto, diferentemente da Regressão Linear, a equação $\\nabla_{w,b} J = 0$ não tem solução analítica para os coeficientes $w$ e $b$ devido à não-linearidade da função logística.\n",
    "\n",
    "Assim, são necessários métodos iterativos para a atualização dos coeficientes. O algoritmo de otimização mais comum é o Gradiente Descente, cujas equações são definidas por:\n",
    "\n",
    "$$\\begin{cases}\n",
    "    w_{t + 1} = w_{t} - \\alpha \\cdot \\frac{\\partial J}{\\partial w} \\\\ \\\\\n",
    "    b_{t + 1} = b_{t} - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "\\end{cases}$$\n",
    "\n",
    "Agora, vamos implementar o código para atualização dos coeficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TREINAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        m = len(y_true)\n",
    "        loss = - (1 / m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "    def _backward(self, X, y_true, y_pred):\n",
    "        m = len(X)\n",
    "        dw = (1 / m) * np.dot(X.T, (y_pred - y_true))\n",
    "        db = (1 / m) * np.sum(y_pred - y_true)\n",
    "        return dw, db\n",
    "\n",
    "    def fit(self, X, y, weights=None, bias=None, learning_rate=1e-3, epochs=100, print_rate=10):\n",
    "        n = X.shape[1]\n",
    "\n",
    "        if weights == None and bias == None:\n",
    "            self.weights = np.zeros(n)\n",
    "            self.bias = 0\n",
    "        else:\n",
    "            self.weights = weights\n",
    "            self.bias = bias\n",
    "\n",
    "        loss_list = []\n",
    "        weights_list = []\n",
    "        bias_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self._forward(X)\n",
    "            loss = self._compute_loss(y, y_pred)\n",
    "            dw, db = self._backward(X, y, y_pred)\n",
    "\n",
    "            self.weights = self.weights - learning_rate * dw\n",
    "            self.bias = self.bias - learning_rate * db\n",
    "\n",
    "            loss_list.append(loss)\n",
    "            weights_list.append(self.weights)\n",
    "            bias_list.append(self.bias)\n",
    "\n",
    "            if epoch % print_rate == 0:\n",
    "                print(f'Epoch {epoch}: Loss {loss:.4f}')\n",
    "\n",
    "        history = {'epoch' : np.arange(epochs) + 1, 'loss' : np.asarray(loss_list), 'weights' : np.asarray(weights_list), 'bias' : np.asarray(bias_list)}\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        y_pred = self._forward(X)\n",
    "        y_pred_class = np.where(y_pred > threshold, 1, 0)\n",
    "        return y_pred_class\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        y_pred = self._forward(X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,2].reshape(-1,1)\n",
    "w, b = np.asarray([-8]), 6\n",
    "\n",
    "learning_rate = 1\n",
    "epochs = 31\n",
    "\n",
    "logreg = Logistic_Regression()\n",
    "\n",
    "history = logreg.fit(x, y, w, b, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.linspace(x.min(), x.max(), 1000)\n",
    "\n",
    "N = 25\n",
    "\n",
    "w_max = b_max = 10\n",
    "w_min = b_min = -w_max\n",
    "\n",
    "w_ = np.linspace(w_min, w_max, N).reshape(1,-1)\n",
    "b_ = np.linspace(b_min, b_max, N).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "Z_ = np.dot(x, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "Y_hat_ = sigmoid(Z_)\n",
    "\n",
    "L_ = log_loss(np.tile(y.reshape(-1,1), N**2), Y_hat_)\n",
    "L_ = L_.reshape(N, N)\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "\n",
    "ax0 = fig.add_subplot(131)\n",
    "ax1 = fig.add_subplot(132)\n",
    "ax2 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "ax0.plot(history['epoch'], history['loss'], c='black', zorder=0)\n",
    "sct0 = ax0.scatter([],[], ec='black', c='orange', s=40, zorder=1)\n",
    "ax0.set_xlabel('Epoch')\n",
    "ax0.set_ylabel('Log-Loss')\n",
    "\n",
    "ax1.scatter(x[y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax1.scatter(x[y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax1_line, = ax1.plot(x_, x_, linestyle='--', c='black')\n",
    "ax1.set_xlabel('$x_{0}$' + f\" ({data['feature_names'][2]})\")\n",
    "ax1.set_ylabel('$y$ (Class)')\n",
    "ax1.set_ylim(-.1, 1.1)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot_surface(W_, B_, L_, cmap='viridis', zorder=0)\n",
    "ax2_line, = ax2.plot([], [], [], marker='o', color='orange', zorder=3)\n",
    "ax2.set_xlabel('$W$')\n",
    "ax2.set_ylabel('$b$')\n",
    "ax2.set_zlabel('Likelihood')\n",
    "\n",
    "def update(epoch):\n",
    "    iepoch = epoch - 1\n",
    "\n",
    "    w = history['weights'][iepoch][0]\n",
    "    b = history['bias'][iepoch]\n",
    "\n",
    "    z_ = w * x_ + b\n",
    "    z = w * x + b\n",
    "    y_hat_ = sigmoid(z_)\n",
    "    y_hat = sigmoid(z)\n",
    "    lh = log_loss(y, y_hat.flatten())\n",
    "    \n",
    "    sct0.set_offsets([epoch, history['loss'][iepoch]])\n",
    "\n",
    "    ax1_line.set_ydata(y_hat_)\n",
    "    ax1_line.set_label(f'w:{w} | b:{b}')\n",
    "    ax1.set_title(f'Likelihood: {lh}')\n",
    "\n",
    "    ax2_line.set_data([w], [b])\n",
    "    ax2_line.set_3d_properties(lh)\n",
    "\n",
    "interact(update, epoch=widgets.IntSlider(value=1, min=1, max=len(history['epoch'])))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FRONTEIRA DE DECISÃO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como visto anteriormente, a função logística (sigmoide) é contínua e seus valores variam entre 0 e 1, estabelecendo uma relação de probabilidade entre as variáveis $P(y=1|X)$. Isto é, o modelo de regressão logística não retorna, por si só, a classe a qual uma determinada amostra $X_{i}$ pertence, mas a probabilidade de esta amostra pertencer à classe $y_{i}=1$. É preciso, então, classificar essa probabilidade predita. É comum definir o valor de $0,5$ como a probabilidade limite entre duas classes. Ou seja, amostras com valores de probabilidade predita maiores que $50\\%$ são classificadas como $y_{i}=1$ e o contrário $y_{i}=0$.\n",
    "\n",
    "O valor desse limiar pode mudar para cada problema. Por exemplo, no caso de classificação de um câncer entre benígno e malígno é preferível reduzir o valor do limiar, \"facilitando\" a classificação de cânceres malígnos. O tratamento de pacientes com cada um desses tipos de câncer deve ser diferente de acordo com à gravidade do problema e, portanto, esta medida, apesar de resultar em mais falsos positivos pode direcionar um tratamento mais eficaz para pacientes com câncer malígno que talvez fossem classificados como benígno. Lembrando que caberá ao médico especialista o diagnóstico final.\n",
    "\n",
    "Para facilitar o entendimento da fronteira de decisão, vamos utilizar agora duas variáveis independentes para a classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature0 = 1\n",
    "feature1 = 2\n",
    "\n",
    "x0, x1 = X[:,feature0], X[:,feature1]\n",
    "x = X[:, [feature0, feature1]]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax0.scatter(x0[y==0], x1[y==0], c='green', ec='black', s=40, label='Benigno')\n",
    "ax0.scatter(x0[y==1], x1[y==1], c='red', ec='black', s=40, label='Maligno')\n",
    "ax0.set_xlabel('$x_{0}$ ' + f'({data[\"feature_names\"][feature0]})')\n",
    "ax0.set_ylabel('$x_{1}$ ' + f'({data[\"feature_names\"][feature1]})')\n",
    "ax0.legend()\n",
    "\n",
    "ax1.scatter(xs=x0[y==0], ys=x1[y==0], zs=y[y==0], c='green', ec='black', s=40, label='Benigno')\n",
    "ax1.scatter(xs=x0[y==1], ys=x1[y==1], zs=y[y==1], c='red', ec='black', s=40, label='Maligno')\n",
    "ax1.set_xlabel('$x_{0}$ ' + f'({data[\"feature_names\"][feature0]})')\n",
    "ax1.set_ylabel('$x_{1}$ ' + f'({data[\"feature_names\"][feature1]})')\n",
    "ax1.set_zlabel('$y$ (class)')\n",
    "ax1.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "epochs = 30\n",
    "\n",
    "logreg = Logistic_Regression()\n",
    "history = logreg.fit(x, y, learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "w, b = history['weights'][-1], history['bias'][-1]\n",
    "\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com a função logística\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{z}} $$\n",
    "\n",
    "$g(z)$ será maior que $0,5$ quando $z > 0$ e vimos que\n",
    "\n",
    "$$ z(X) = w \\cdot X + b $$\n",
    "\n",
    "Isto é\n",
    "\n",
    "$$ z(X) > 0 \\leftrightarrow w \\cdot X + b > 0 $$\n",
    "\n",
    "Substituindo os valores de $w$ e $b$ obtidos no treinamento, temos\n",
    "\n",
    "$$ \\begin{align*}\n",
    "z(X) > 0 & \\leftrightarrow w_{0} \\cdot x_{0} + w_{1} \\cdot x_{1} + b > 0 \\\\ \\\\\n",
    "& \\leftrightarrow 0,77 \\cdot x_{0} + 2,56 \\cdot x_{1} + (-0,66) > 0 \\\\ \\\\\n",
    "& \\leftrightarrow x_{1} > -\\frac{-0,66 + 0,77 \\cdot x_{0}}{2,56}\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_points(X0, w, b):\n",
    "    w0, w1 = w[0], w[1]\n",
    "    line = [(-b - w0 * x0) / w1 for x0 in X0]\n",
    "    return line\n",
    "\n",
    "def boundary_surface(X0, w, b):\n",
    "    X0_ = np.asarray([X0, X0])\n",
    "    X1_ = np.asarray([boundary_points(X0, w, b), boundary_points(X0, w, b)])\n",
    "    Z_ = np.asarray([[0, 0], [1, 1]])\n",
    "    return X0_, X1_, Z_\n",
    "\n",
    "    # [[x0_min, x0_max],    [[boundary_points0, boundary_points0],\n",
    "    #  [x0_min, x0_max]]     [boundary_points1, boundary_points1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sfc_X, b_sfc_Y, b_sfc_Z = boundary_surface([x0.min(), x0.max()], w, b)\n",
    "\n",
    "w0, w1 = w[0], w[1]\n",
    "\n",
    "n_grid = 50\n",
    "x0_ = np.linspace(x0.min(), x0.max(), n_grid)\n",
    "x1_ = np.linspace(x1.min(), x1.max(), n_grid)\n",
    "\n",
    "X0_, X1_ = np.meshgrid(x0_, x1_)\n",
    "X_ = np.concatenate([X0_.reshape(-1,1), X1_.reshape(-1,1)], axis=1)\n",
    "\n",
    "Y_ = logreg.predict_proba(X_)\n",
    "Y_ = Y_.reshape(n_grid, n_grid)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d', computed_zorder=False)\n",
    "\n",
    "cmap = 'Blues'\n",
    "\n",
    "img = ax0.pcolormesh(x0_, x1_, Y_, cmap=cmap)\n",
    "ax0.scatter(x[y==0, 0], x[y==0, 1], c='green', ec='black', s=40, label='Benigno')\n",
    "ax0.scatter(x[y==1, 0], x[y==1, 1], c='red', ec='black', s=40, label='Maligno')\n",
    "ax0.plot([x0.min(), x0.max()], boundary_points([x0.min(), x0.max()], w, b), linestyle='--', color='yellow')\n",
    "ax0.set_xlabel('$x_{0}$ ' + f'({data[\"feature_names\"][feature0]})')\n",
    "ax0.set_ylabel('$x_{1}$ ' + f'({data[\"feature_names\"][feature1]})')\n",
    "ax0.legend()\n",
    "\n",
    "sfc = ax1.plot_surface(X0_, X1_, Y_, cmap=cmap)\n",
    "ax1.plot_surface(b_sfc_X, b_sfc_Y, b_sfc_Z, color='yellow', alpha=0.5)\n",
    "ax1.scatter(xs=x[y==0, 0], ys=x[y==0, 1], zs=y[y==0], c='green', ec='black', s=40, label='Benigno')\n",
    "ax1.scatter(xs=x[y==1, 0], ys=x[y==1, 1], zs=y[y==1], c='red', ec='black', s=40, label='Maligno')\n",
    "ax1.set_xlabel('$x_{0}$ ' + f'({data[\"feature_names\"][feature0]})')\n",
    "ax1.set_ylabel('$x_{1}$ ' + f'({data[\"feature_names\"][feature1]})')\n",
    "ax1.set_zlabel('$y$ (class)')\n",
    "ax1.legend()\n",
    "\n",
    "# fig.colorbar(img, label='Probabilidade')\n",
    "fig.colorbar(sfc, label='Probabilidade')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sfc_X, b_sfc_Y, b_sfc_Z = boundary_surface([x0.min(), x0.max()], w, b)\n",
    "\n",
    "w0, w1 = w[0], w[1]\n",
    "\n",
    "n_grid = 50\n",
    "x0_ = np.linspace(x0.min(), x0.max(), n_grid)\n",
    "x1_ = np.linspace(x1.min(), x1.max(), n_grid)\n",
    "\n",
    "X0_, X1_ = np.meshgrid(x0_, x1_)\n",
    "X_ = np.concatenate([X0_.reshape(-1,1), X1_.reshape(-1,1)], axis=1)\n",
    "\n",
    "Y_ = logreg.predict_proba(X_)\n",
    "Y_ = Y_.reshape(n_grid, n_grid)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d', computed_zorder=False)\n",
    "\n",
    "cmap = 'Blues'\n",
    "\n",
    "img = ax0.pcolormesh(x0_, x1_, Y_, cmap=cmap)\n",
    "ax0.scatter(x[y==0, 0], x[y==0, 1], c='green', ec='black', s=40, label='Benigno')\n",
    "ax0.scatter(x[y==1, 0], x[y==1, 1], c='red', ec='black', s=40, label='Maligno')\n",
    "ax0.plot([x0.min(), x0.max()], boundary_points([x0.min(), x0.max()], w, b), linestyle='--', color='yellow')\n",
    "ax0.set_xlabel('$x_{0}$ ' + f'({data[\"feature_names\"][feature0]})')\n",
    "ax0.set_ylabel('$x_{1}$ ' + f'({data[\"feature_names\"][feature1]})')\n",
    "ax0.legend()\n",
    "\n",
    "sfc = ax1.plot_surface(X0_, X1_, Y_, cmap=cmap)\n",
    "ax1.plot_surface(b_sfc_X, b_sfc_Y, b_sfc_Z, color='yellow', alpha=0.5)\n",
    "ax1.scatter(xs=x[y==0, 0], ys=x[y==0, 1], zs=y[y==0], c='green', ec='black', s=40, label='Benigno')\n",
    "ax1.scatter(xs=x[y==1, 0], ys=x[y==1, 1], zs=y[y==1], c='red', ec='black', s=40, label='Maligno')\n",
    "ax1.set_xlabel('$x_{0}$ ' + f'({data[\"feature_names\"][feature0]})')\n",
    "ax1.set_ylabel('$x_{1}$ ' + f'({data[\"feature_names\"][feature1]})')\n",
    "ax1.set_zlabel('$y$ (class)')\n",
    "ax1.legend()\n",
    "\n",
    "# fig.colorbar(img, label='Probabilidade')\n",
    "fig.colorbar(sfc, label='Probabilidade')\n",
    "\n",
    "def update(w0, w1, b):\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
