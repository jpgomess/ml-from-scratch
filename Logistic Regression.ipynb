{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why noy use linear regression\n",
    "\n",
    "# Logistic hypothesis\n",
    "\n",
    "# Parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTAÇÕES NECESSÁRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from ipywidgets import interact\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot([1,2,3],[2,4,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRIÇÃO DO PROBLEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que temos uma relação de pacientes com câncer de mama e precisamos desenvolver um modelo de Aprendizado de Máquina para prever qual a gravidade do câncer destes pacientes. Entre as respostas possíveis, temos que classificar o câncer como **maligno** ou **benigno**.\n",
    "\n",
    "Dessa forma, o problema não se encaixa como um problema de regressão, mas sim de classificação. Ou seja, queremos prever uma variável qualitativa, como uma categoria (**maligno** ou **benigno**).\n",
    "\n",
    "Para que os computadores possam \"entender\" as variáveis qualitativas, é preciso traduzi-las para a linguagem numérica, ex.: benigno = 0; e maligno = 1. Este processo de tradução é chamado de ***encoding*** ou **codificação**.\n",
    "\n",
    "Veremos mais à frente que estes valores qualitativos podem ser interpretados como a probabilidade de uma determinada amostra pertencer a classe-alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARREGAMENTO E VISUALIZAÇÃO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer(as_frame=True)\n",
    "\n",
    "print(f\"{'-'*80}\\nVariáveis independentes: {data['feature_names']} \\n{'-'*80}\\nVariáveis dependentes: {data['target_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"-\"*100} \\nX:')\n",
    "display(data['data'].head(5))\n",
    "\n",
    "print(f'{\"-\"*100} \\ny:')\n",
    "display(data['target'][-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao analisar a base de dados, observa-se que o valor correspondente à classe **maligno** é 0 e aquele correspondente à classe **benigno** é 1. É comum em um problema de classificação binária (duas classes-alvo) que a classe principal a qual se deseja prever seja representada pelo número 1. Dessa forma, por questões didáticas, decidi trocar os valores entre as classes. Ou seja, agora o valor referente à classe **maligno** (classe principal) será 1 e o da classe **benigno** será 0.\n",
    "\n",
    "**obs: essa mudança de ordem não altera em nada o processo de desenvolvimento e otimização do modelo de regressão logística.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['data'].values, data['target'].values\n",
    "\n",
    "y = np.where(y == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.scatter(X[:,2][y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='benigno')\n",
    "ax0.scatter(X[:,2][y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='maligno')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POR QUE NÃO USAR REGRESSÃO LINEAR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez transformadas as classes em valores binários 0/1, podemos treinar um regressor linear para estimar qual a categoria do câncer de um determinado paciente da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.linspace(X[:,2].min(), X[:,2].max(), 10)\n",
    "y_ = x_ * (1 / 40) - 2 # Linear Regressor\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.scatter(X[:,2][y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(X[:,2][y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0.plot(x_, y_, linestyle='--', c='black', label='Regressor Linear')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando a figura anterior, podemos interpretar os valores preditos pelo regressor linear como uma estimativa de $P(y=1|X)$. Isto é, a probabilidade de uma determinada amostra $X$ pertencer a classe 1 (maligno). Observa-se que os valores estimados pelo regressor linear ultrapassam os limites de probabilidae (0 e 1) e podem variar de $-\\infty$ a $+\\infty$.\n",
    "\n",
    "Portanto, é por essas e outras razões que é preferível a utilização de um regressor mais adequado capaz de lidar com valores qualitativos, como o modelo de **Regressão Logística**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESSÃO LOGÍSTICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo de Regressão Logística tem esse nome devido à função que serve de hipótese para o modelo: função logística, mais especificamente a função sigmóide.\n",
    "\n",
    "A imagem da função sigmóide varia entre os valores 0 e 1. Sua equação é descrita da seguinte forma:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Veremos mais à frente que, quando utilizada para modelar um problema de classificação do tipo $P(y=1|X)$, a função precisa ser ajustada e $z$ passa a ser uma abreviação de $w \\cdot X + b$ e a versão completa da função é, então:\n",
    "\n",
    "$$ g_{W, b}(X) = \\frac{1}{1 + e^{-(W \\cdot X + b)}} $$\n",
    "\n",
    "onde $g_{W, b}(X) = P(y=1|X)$ é a variável dependente (saída), $X$ são as variáveis independentes (entrada), $W$ e $b$ são parâmetros a serem otimizados.\n",
    "\n",
    "**OBS: para facilitar, a partir de agora, vamos abreviar $g_{W, b}(X)$ por $\\hat{y}$, onde entende-se que $g(.)$ é a resposta calculada pelo modelo ($g(.)=\\hat{y}$)**\n",
    "\n",
    "A forma em \"S\" da função sigmoide pode ser visualizada na figura a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10, 10, 100)\n",
    "g = sigmoid(z)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax0.plot(z, g, c='black', label='Sigmoid')\n",
    "ax0.set_xlabel('$z$')\n",
    "ax0.set_ylabel('$g(z)$')\n",
    "ax0.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos tentar implementar a função logística para receber como entrada as variáveis do nosso problema de classificação de câncer de mama e ver como os parâmetros influenciam a função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizar utils\n",
    "\n",
    "def standardize(x):\n",
    "    return (x - x.mean(axis=0)) / x.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.linspace(X[:,2].min(), X[:,2].max(), 1000)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "ax0 = fig.add_subplot(1,1,1)\n",
    "\n",
    "def update(w, b):\n",
    "\n",
    "    z = w * x_ + b\n",
    "    g = sigmoid(z)\n",
    "\n",
    "    ax0.clear()\n",
    "    ax0.scatter(X[:,2][y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "    ax0.scatter(X[:,2][y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "    ax0.plot(x_, g, linestyle='--', c='black', label=f'w={round(w, 2)} | b={round(b, 2)}')\n",
    "    ax0.set_xlabel(data['feature_names'][2])\n",
    "    ax0.set_ylabel('Class')\n",
    "    ax0.legend()\n",
    "\n",
    "interact(update, w=widgets.FloatSlider(value=0, min=-10, max=10), b=widgets.FloatSlider(value=0, min=-10, max=10))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTIMIZAÇÃO DOS PARÂMETROS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando desejamos modelar um regressor logístico para um problema de classificação, estimamos uma função logística cujos coeficientes $W$ e $b$ são desconhecidos. Portanto, é necessário estimar os coeficientes de maneira que a curva melhor se ajuste aos dados de treinamento.\n",
    "\n",
    "Para a otimização dos coeficientes do modelo de Regressão Linear, é utilizado o método dos Mínimos Quadrados para minimizar a diferença entre os dados preditos $\\hat{y}$ e reais $y$ da seguinte forma:\n",
    "\n",
    "$$ MSE = \\frac{1}{2m} \\sum_{i=0}^{m-1} (\\hat{y_{i}} - y_{i})^{2} $$\n",
    "\n",
    "A utilização da MSE como função custo da Regressão Logística não é indicada e abaixo demonstraremos o porquê:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y):\n",
    "    return np.mean((y_pred - y)**2, axis=0) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 25\n",
    "\n",
    "w_ = np.linspace(-25, 25, n_grid).reshape(1,-1)\n",
    "b_ = np.linspace(-25, 25, n_grid).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "x_ = X[:, 2].reshape(-1,1)\n",
    "z_ = np.dot(x_, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "y_pred = sigmoid(z_)\n",
    "\n",
    "mse_ = mse(y_pred, np.tile(y.reshape(-1,1), n_grid**2))\n",
    "mse_ = mse_.reshape(n_grid, n_grid)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "ax0 = fig.add_subplot(111, projection='3d')\n",
    "ax0.plot_surface(W_, B_, mse_, cmap='viridis')\n",
    "ax0.set_xlabel('$W$')\n",
    "ax0.set_ylabel('$b$')\n",
    "ax0.set_zlabel('$MSE$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido ao fato de a função logística (sigmoide) ser não-linear, a função MSE, quando utilizada no contexto da Regressão Logística, assume uma forma não-convexa. Isto é, existem vários pontos da função custo diferentes do mínimo global que também possuem derivadas parciais com relação aos parâmetros iguais a 0. Dessa forma, a otimização dos parâmetros através do metódo Gradiente Descendente é prejudicada, uma vez que quando o gradiente é nulo não há atualização.\n",
    "\n",
    "Ao invés de utilizar a MSE, é utilizada uma abordagem diferente: da Probabilidade Máxima (*Maximum Likelihood*). A estimativa dos coeficientes de um modelo de Regressão Logística é dada de tal forma a maximizar as probabilidades de classificação em cada classe.\n",
    "\n",
    "$ \\begin{equation}\n",
    "    \\begin{align*}\n",
    "        \\text{Likelihood} = L(y, \\hat{y}) & = \\prod_{i=0}^{m-1} P(y_{i}=1|X_{i})^{y_{i}} \\prod_{i=0}^{m-1} P(y_{i}=0|X_{i})^{1-y_{i}} \\\\\n",
    "        & = \\prod_{i=0}^{m-1} P(y_{i}=1|X_{i})^{y_{i}} \\prod_{i=0}^{m-1} \\left[ 1 - P(y_{i}=1|X_{i}) \\right]^{1-y_{i}} \\\\\n",
    "        & = \\prod_{i=0}^{m-1} \\hat{y_{i}}^{y_{i}} \\prod_{i=0}^{m-1} \\left( 1 - \\hat{y_{i}} \\right)^{1-y_{i}}\n",
    "    \\end{align*}\n",
    "\\end{equation} $\n",
    "\n",
    "Através do produtório das probabilidades preditas para cada classe, a função *Likelihood* mede o nível de acerto do modelo em uma escala de probabilidade $[0, 1]$. Na prática, a probabilidade predita ($\\hat{y}$) nunca será igual a 0 ou 1, de fato, porque a função sigmoid nunca alcança esses valores. Entretanto, quanto mais perto as probabilidades preditas forem das verdadeiras, maior será o valor do *Likelihood*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(y, y_hat):\n",
    "    return np.prod(y_hat**y * (1 - y_hat)**(1-y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,2].reshape(-1,1)\n",
    "x_ = np.linspace(x.min(), x.max(), 1000)\n",
    "\n",
    "N = 25\n",
    "\n",
    "w_max = b_max = 10\n",
    "w_min = b_min = -w_max\n",
    "\n",
    "w_ = np.linspace(w_min, w_max, N).reshape(1,-1)\n",
    "b_ = np.linspace(b_min, b_max, N).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "Z_ = np.dot(x, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "Y_hat_ = sigmoid(Z_)\n",
    "\n",
    "L_ = likelihood(np.tile(y.reshape(-1,1), N**2), Y_hat_)\n",
    "L_ = L_.reshape(N, N)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax0.scatter(x[y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(x[y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0_line, = ax0.plot(x_, x_, linestyle='--', c='black')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.set_ylim(-.1, 1.1)\n",
    "ax0.legend()\n",
    "\n",
    "ax1.plot_surface(W_, B_, L_, cmap='viridis', zorder=0)\n",
    "ax1_line, = ax1.plot([], [], [], marker='o', color='orange', zorder=3)\n",
    "ax1.set_xlabel('$W$')\n",
    "ax1.set_ylabel('$b$')\n",
    "ax1.set_zlabel('Likelihood')\n",
    "\n",
    "def update(w, b):\n",
    "\n",
    "    z_ = w * x_ + b\n",
    "    z = w * x + b\n",
    "    y_hat_ = sigmoid(z_)\n",
    "    y_hat = sigmoid(z)\n",
    "    lh = likelihood(y, y_hat.flatten())\n",
    "    \n",
    "    ax0_line.set_ydata(y_hat_)\n",
    "    ax0_line.set_label(f'w:{w} | b:{b}')\n",
    "    ax0.set_title(f'Likelihood: {lh}')\n",
    "\n",
    "    ax1_line.set_data([w], [b])\n",
    "    ax1_line.set_3d_properties(lh)\n",
    "\n",
    "interact(update, w=widgets.FloatSlider(value=1, min=w_min, max=w_max), b=widgets.FloatSlider(value=0, min=b_min, max=b_max))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando os resultados da função *Likelihood*, tem-se que, devido ao produtório de uma série de valores menores que 1 e maiores que 0, o valor da função é, para a maioria dos coeficientes, próximo de zero e, portanto, métodos que dependem do gradiente para a atualização dos coeficientes não funcionarão.\n",
    "\n",
    "Para contornar esse problema, a função logarítmica é aplicada à função *Likelihood*: \n",
    "\n",
    "$$ \\begin{equation*}\n",
    "    log \\left(L(y, \\hat{y}) \\right) = log\\left(\\prod_{i=0}^{m-1} \\hat{y_{i}}^{y_{i}} \\prod_{i=0}^{m-1} \\left( 1 - \\hat{y_{i}} \\right)^{1-y_{i}}\\right)\n",
    "\\end{equation*} $$\n",
    "\n",
    "Aplicando a seguinte propriedade do logarítmo: $log(A \\cdot B)=log(A)+log(B)$ e chamando $log(L(y, \\hat{y}))$ de $LL(y, \\hat{y})$ temos que\n",
    "\n",
    "$$ \\begin{equation*}\n",
    "    \\begin{align*}\n",
    "        LL(y, \\hat{y}) & = \\sum_{i=0}^{m-1} log(\\hat{y_{i}}^{y_{i}}) + log([1 - \\hat{y_{i}}]^{1 - y_{i}}) \\\\\n",
    "        & = \\sum_{i=0}^{m-1} y_{i} \\cdot log(\\hat{y_{i}}) + (1 - y_{i}) log([1 - \\hat{y_{i}}]) \\\\\n",
    "    \\end{align*}\n",
    "\\end{equation*} $$\n",
    "\n",
    "Com isso, temos a expressão para o *log-likelihood*. Sua forma pode ser visualizada na figura abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(y, y_hat):\n",
    "    return np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,2].reshape(-1,1)\n",
    "x_ = np.linspace(x.min(), x.max(), 1000)\n",
    "\n",
    "N = 25\n",
    "\n",
    "w_max = b_max = 10\n",
    "w_min = b_min = -w_max\n",
    "\n",
    "w_ = np.linspace(w_min, w_max, N).reshape(1,-1)\n",
    "b_ = np.linspace(b_min, b_max, N).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "Z_ = np.dot(x, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "Y_hat_ = sigmoid(Z_)\n",
    "\n",
    "L_ = log_likelihood(np.tile(y.reshape(-1,1), N**2), Y_hat_)\n",
    "L_ = L_.reshape(N, N)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax0.scatter(x[y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(x[y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0_line, = ax0.plot(x_, x_, linestyle='--', c='black')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.set_ylim(-.1, 1.1)\n",
    "ax0.legend()\n",
    "\n",
    "ax1.plot_surface(W_, B_, L_, cmap='viridis', zorder=0)\n",
    "ax1_line, = ax1.plot([], [], [], marker='o', color='orange', zorder=3)\n",
    "ax1.set_xlabel('$W$')\n",
    "ax1.set_ylabel('$b$')\n",
    "ax1.set_zlabel('Likelihood')\n",
    "\n",
    "def update(w, b):\n",
    "\n",
    "    z_ = w * x_ + b\n",
    "    z = w * x + b\n",
    "    y_hat_ = sigmoid(z_)\n",
    "    y_hat = sigmoid(z)\n",
    "    lh = log_likelihood(y, y_hat.flatten())\n",
    "    \n",
    "    ax0_line.set_ydata(y_hat_)\n",
    "    ax0_line.set_label(f'w:{w} | b:{b}')\n",
    "    ax0.set_title(f'Likelihood: {lh}')\n",
    "\n",
    "    ax1_line.set_data([w], [b])\n",
    "    ax1_line.set_3d_properties(lh)\n",
    "\n",
    "interact(update, w=widgets.FloatSlider(value=1, min=w_min, max=w_max), b=widgets.FloatSlider(value=0, min=b_min, max=b_max))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a otimização dos coeficientes da Regressão linear é comum utilizar métodos de minimização de funções, em detrimento daqueles de maximização. Portanto, na realidade, a função cuja **minimização** significa encontrar os melhores coeficientes (função custo) é o negativo do *log-likelihood*:\n",
    "\n",
    "$$ -L(y, \\hat{y}) = J(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m-1} -y_{i} \\cdot log(\\hat{y_{i}}) -(1 - y_{i}) \\cdot log(1 - \\hat{y_{i}}) $$\n",
    "\n",
    "A forma de $J(y, \\hat{y})$ pode ser visualizada na figura abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,2].reshape(-1,1)\n",
    "x_ = np.linspace(x.min(), x.max(), 1000)\n",
    "\n",
    "N = 25\n",
    "\n",
    "w_max = b_max = 10\n",
    "w_min = b_min = -w_max\n",
    "\n",
    "w_ = np.linspace(w_min, w_max, N).reshape(1,-1)\n",
    "b_ = np.linspace(b_min, b_max, N).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "Z_ = np.dot(x, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "Y_hat_ = sigmoid(Z_)\n",
    "\n",
    "L_ = log_loss(np.tile(y.reshape(-1,1), N**2), Y_hat_)\n",
    "L_ = L_.reshape(N, N)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax0.scatter(x[y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(x[y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0_line, = ax0.plot(x_, x_, linestyle='--', c='black')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.set_ylim(-.1, 1.1)\n",
    "ax0.legend()\n",
    "\n",
    "ax1.plot_surface(W_, B_, L_, cmap='viridis', zorder=0)\n",
    "ax1_line, = ax1.plot([], [], [], marker='o', color='orange', zorder=3)\n",
    "ax1.set_xlabel('$W$')\n",
    "ax1.set_ylabel('$b$')\n",
    "ax1.set_zlabel('Likelihood')\n",
    "\n",
    "def update(w, b):\n",
    "\n",
    "    z_ = w * x_ + b\n",
    "    z = w * x + b\n",
    "    y_hat_ = sigmoid(z_)\n",
    "    y_hat = sigmoid(z)\n",
    "    lh = log_loss(y, y_hat.flatten())\n",
    "    \n",
    "    ax0_line.set_ydata(y_hat_)\n",
    "    ax0_line.set_label(f'w:{w} | b:{b}')\n",
    "    ax0.set_title(f'Likelihood: {lh}')\n",
    "\n",
    "    ax1_line.set_data([w], [b])\n",
    "    ax1_line.set_3d_properties(lh)\n",
    "\n",
    "interact(update, w=widgets.FloatSlider(value=1, min=w_min, max=w_max), b=widgets.FloatSlider(value=0, min=b_min, max=b_max))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a obtenção da função custo para a Regressão Logística, podemos observar que a função resultante é agora convexa e suave. Além de os valores estarem adequados, nem muito pequenos, nem muito grandes.\n",
    "\n",
    "Encontrar os parâmetros que minimizem essa função significa encontrar aqueles que satisfaçam a equação:\n",
    "\n",
    "$$ \\nabla_{w,b} J(y, \\hat{y}) = \\frac{\\partial J}{\\partial w}\\textbf{i} + \\frac{\\partial J}{\\partial b}\\textbf{j} = 0 \\Rightarrow \\begin{cases}\n",
    "                \\frac{\\partial J}{\\partial w} = 0 \\\\ \\\\\n",
    "                \\frac{\\partial J}{\\partial b} = 0\n",
    "            \\end{cases} $$\n",
    "\n",
    "Para isso, então, é necessário calcular as derivadas parciais da função custo com relação aos coeficientes $w$ e $b$.\n",
    "\n",
    "Primeiro, vamos calcular a derivada com relação a $w$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial w} & = \\frac{\\partial}{\\partial w} \\left[\\frac{1}{m} \\sum_{i=0}^{m-1} - y_{i} \\cdot log(\\hat{y_{i}}) - (1 - y_{i}) \\cdot log(1 - \\hat{y_{i}}) \\right] \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} - y_{i} \\cdot \\frac{\\partial}{\\partial w} log(\\hat{y_{i}}) - (1 - y_{i}) \\cdot \\frac{\\partial}{\\partial w} log(1 - \\hat{y_{i}})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Calculando $\\frac{\\partial}{\\partial w} log(\\hat{y_{i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial w} log(\\hat{y_{i}}) = \\frac{\\partial}{\\partial w} log\\left(\\frac{1}{1 + e^{wx+b}}\\right) $$\n",
    "\n",
    "Para isso, vamos fazer as seguintes substituições de variáveis:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    g = log(\\frac{1}{1 + e^{wx + b}}) = log(\\hat{y}) \\\\\n",
    "    k = \\frac{1}{1 + e^{wx + b}} = \\hat{y} \\\\\n",
    "    v = 1 + e^{wx + b} = \\hat{y}^{-1} \\\\\n",
    "    u = wx + b\n",
    "\\end{cases} $$\n",
    "\n",
    "Dessa forma, de acordo com a regra da cadeia, temos que $\\frac{\\partial g}{\\partial w} = \\frac{\\partial g}{\\partial k} \\cdot \\frac{\\partial k}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u} \\cdot \\frac{\\partial u}{\\partial w}$.\n",
    "\n",
    "Resolvendo as derivadas parciais, temos\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    \\frac{\\partial g}{\\partial k} = \\frac{1}{k} = \\hat{y}^{-1} \\\\\n",
    "    \\frac{\\partial k}{\\partial v} = -v^{-2} = -\\hat{y}^{2} \\\\\n",
    "    \\frac{\\partial v}{\\partial u} = e^{u} = \\hat{y}^{-1} - 1 \\\\\n",
    "    \\frac{\\partial u}{\\partial w} = x\n",
    "\\end{cases} $$\n",
    "\n",
    "Substituindo, temos\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    \\frac{\\partial g}{\\partial w} & = \\frac{\\partial g}{\\partial k} \\cdot \\frac{\\partial k}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u} \\cdot \\frac{\\partial u}{\\partial w} \\\\\n",
    "    & = \\hat{y}^{-1} \\cdot (-\\hat{y}^{2}) \\cdot (\\hat{y}^{-1} - 1) \\cdot x \\\\\n",
    "    & = -\\hat{y} \\cdot (\\hat{y}^{-1} - 1) \\cdot x \\\\\n",
    "    & = (\\hat{y} - 1) \\cdot x\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Calculando $\\frac{\\partial}{\\partial w} log(1 - \\hat{y_{i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial w} log( 1 - \\hat{y_{i}}) = \\frac{\\partial}{\\partial w} log\\left(1 - \\frac{1}{1 + e^{wx+b}}\\right) $$\n",
    "\n",
    "Para isso, vamos fazer as seguintes substituições de variáveis:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    g = log\\left(1 - \\frac{1}{1 + e^{wx+b}}\\right) = log(1 - \\hat{y}) \\\\\n",
    "    k = 1 - \\frac{1}{1 + e^{wx + b}} = 1 - \\hat{y} \\\\\n",
    "    v = 1 + e^{wx + b} = \\hat{y}^{-1} \\\\\n",
    "    u = wx + b\n",
    "\\end{cases} $$\n",
    "\n",
    "Dessa forma, de acordo com a regra da cadeia, temos que $\\frac{\\partial g}{\\partial w} = \\frac{\\partial g}{\\partial k} \\cdot \\frac{\\partial k}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u} \\cdot \\frac{\\partial u}{\\partial w}$.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolvendo as derivadas parciais, temos\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    \\frac{\\partial g}{\\partial k} = \\frac{1}{k} = \\frac{1}{1 - \\hat{y}} \\\\\n",
    "    \\frac{\\partial k}{\\partial v} = v^{-2} = \\hat{y}^{2} \\\\\n",
    "    \\frac{\\partial v}{\\partial u} = e^{u} = \\hat{y}^{-1} - 1 \\\\\n",
    "    \\frac{\\partial u}{\\partial w} = x\n",
    "\\end{cases} $$\n",
    "\n",
    "Substituindo, temos\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    \\frac{\\partial g}{\\partial w} & = \\frac{\\partial g}{\\partial k} \\cdot \\frac{\\partial k}{\\partial v} \\cdot \\frac{\\partial v}{\\partial u} \\cdot \\frac{\\partial u}{\\partial w} \\\\\n",
    "    & = \\frac{1}{1 - \\hat{y}} \\cdot \\hat{y}^{2} \\cdot (\\hat{y}^{-1} - 1) \\cdot x \\\\\n",
    "    & = \\frac{\\hat{y} \\cdot (1 - \\hat{y})}{1 - \\hat{y}} \\cdot x \\\\\n",
    "    & = \\hat{y} \\cdot x\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituindo os resultados de $\\frac{\\partial}{\\partial w} log(\\hat{y_{i}})$ e $\\frac{\\partial}{\\partial w} log(1 - \\hat{y_{i}})$ em $\\frac{\\partial J}{\\partial w}$, temos\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial w} & = \\frac{1}{m} \\sum_{i=0}^{m-1} - y_{i} \\cdot \\frac{\\partial}{\\partial w} log(\\hat{y_{i}}) - (1 - y_{i}) \\cdot \\frac{\\partial}{\\partial w} log(1 - \\hat{y_{i}}) \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} - y_{i} \\cdot (\\hat{y}_{i} - 1) \\cdot x_{i} - (1 - y_{i}) \\cdot \\hat{y}_{i} \\cdot x_{i} \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} [- y_{i} \\cdot (\\hat{y}_{i} - 1) - (1 - y_{i}) \\cdot \\hat{y}_{i}] \\cdot x_{i} \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} (-y_{i} \\hat{y_{i}} + y_{i} - \\hat{y}_{i} + y_{i} \\hat{y}_{i}) \\cdot x_{i} \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=0}^{m-1} (y_{i} - \\hat{y}_{i}) \\cdot x_{i}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo os mesmos passos anteriores para $\\frac{\\partial J}{\\partial b}$ temos ao final:\n",
    "\n",
    "$$ \\begin{cases}\n",
    "    \\frac{\\partial J}{\\partial w} = (y_{i} - \\hat{y}_{i}) \\cdot x_{i} \\\\ \\\\\n",
    "    \\frac{\\partial J}{\\partial b} = (y_{i} - \\hat{y}_{i})\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As equações das derivadas parciais obtidas com relação ao valor predito $\\hat{y}$ são idênticas às da Regressão Linear. Entretanto, diferentemente da Regressão Linear, a equação $\\nabla_{w,b} J = 0$ não tem solução analítica para os coeficientes $w$ e $b$ devido à não-linearidade da função logística.\n",
    "\n",
    "Assim, são necessários métodos iterativos para a atualização dos coeficientes. O algoritmo de otimização mais comum é o Gradiente Descente, cujas equações são definidas por:\n",
    "\n",
    "$$\\begin{cases}\n",
    "    w_{t + 1} = w_{t} - \\alpha \\cdot \\frac{\\partial J}{\\partial w} \\\\ \\\\\n",
    "    b_{t + 1} = b_{t} - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "\\end{cases}$$\n",
    "\n",
    "Agora, vamos implementar o código para atualização dos coeficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TREINAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, y_hat):\n",
    "    m, n = X.shape\n",
    "    err = y - y_hat\n",
    "    print(err.shape, X.shape, y.shape, y_hat.shape)\n",
    "    dj_dw = np.dot(err.reshape(1,-1), X) / m\n",
    "    dj_db = np.mean(err, axis=0)\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, w, b, learning_rate=0.001, epochs=100):\n",
    "    history = {'epoch':[], 'loss':[], 'w':[], 'b':[]}\n",
    "    for epoch in range(epochs):\n",
    "        print(w)\n",
    "        z = np.dot(X, w) + b\n",
    "        y_hat = sigmoid(z)\n",
    "\n",
    "        dj_dw, dj_db = compute_gradients(X, y, y_hat)\n",
    "        w = w + learning_rate * dj_dw\n",
    "        b = b + learning_rate * dj_db\n",
    "\n",
    "        loss = log_loss(y, y_hat.flatten())\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Loss: {loss}')\n",
    "\n",
    "        history['epoch'] += [epoch]\n",
    "        history['loss'] += [loss]\n",
    "        history['b'] += [w]\n",
    "        history['w'] += [b]\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,2].reshape(-1,1)\n",
    "w, b = np.zeros(1), 0\n",
    "\n",
    "history = fit(x, y, w, b, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.linspace(x.min(), x.max(), 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[:,2].reshape(-1,1)\n",
    "x_ = np.linspace(x.min(), x.max(), 1000)\n",
    "\n",
    "N = 25\n",
    "\n",
    "w_max = b_max = 10\n",
    "w_min = b_min = -w_max\n",
    "\n",
    "w_ = np.linspace(w_min, w_max, N).reshape(1,-1)\n",
    "b_ = np.linspace(b_min, b_max, N).reshape(1,-1)\n",
    "W_, B_ = np.meshgrid(w_, b_)\n",
    "\n",
    "Z_ = np.dot(x, W_.reshape(1,-1)) + B_.reshape(1,-1)\n",
    "Y_hat_ = sigmoid(Z_)\n",
    "\n",
    "L_ = log_loss(np.tile(y.reshape(-1,1), N**2), Y_hat_)\n",
    "L_ = L_.reshape(N, N)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax0 = fig.add_subplot(121)\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax0.scatter(x[y==0], y[y==0], c='green', ec='black', s=80, marker='o', label='Benigno')\n",
    "ax0.scatter(x[y==1], y[y==1], c='red', ec='black', s=80, marker='X', label='Maligno')\n",
    "ax0_line, = ax0.plot(x_, x_, linestyle='--', c='black')\n",
    "ax0.set_xlabel(data['feature_names'][2])\n",
    "ax0.set_ylabel('Class')\n",
    "ax0.set_ylim(-.1, 1.1)\n",
    "ax0.legend()\n",
    "\n",
    "ax1.plot_surface(W_, B_, L_, cmap='viridis', zorder=0)\n",
    "ax1_line, = ax1.plot([], [], [], marker='o', color='orange', zorder=3)\n",
    "ax1.set_xlabel('$W$')\n",
    "ax1.set_ylabel('$b$')\n",
    "ax1.set_zlabel('Likelihood')\n",
    "\n",
    "def update(w, b):\n",
    "\n",
    "    z_ = w * x_ + b\n",
    "    z = w * x + b\n",
    "    y_hat_ = sigmoid(z_)\n",
    "    y_hat = sigmoid(z)\n",
    "    lh = log_loss(y, y_hat.flatten())\n",
    "    \n",
    "    ax0_line.set_ydata(y_hat_)\n",
    "    ax0_line.set_label(f'w:{w} | b:{b}')\n",
    "    ax0.set_title(f'Likelihood: {lh}')\n",
    "\n",
    "    ax1_line.set_data([w], [b])\n",
    "    ax1_line.set_3d_properties(lh)\n",
    "\n",
    "interact(update, w=widgets.FloatSlider(value=1, min=w_min, max=w_max), b=widgets.FloatSlider(value=0, min=b_min, max=b_max))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
