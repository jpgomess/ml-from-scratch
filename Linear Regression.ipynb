{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from ipywidgets import interact\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na Regressão Linear, é tentado aproximar a relação entre as variáveis independentes $x$ e as dependentes $y$ através de uma equação linear, do tipo\n",
    "\n",
    "$$ \\hat{y} = \\beta_1 \\cdot x + \\beta_0 $$\n",
    "\n",
    "onde $\\hat{y}$ é a resposta calculada e $\\hat{y} \\approx y$, $x$ é a variável independente e os coeficientes $\\beta_0$ e $\\beta_1$ são os coeficientes linear e angular da reta.\n",
    "\n",
    "Veja no exemplo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "      <th>total-spend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "      <td>337.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "      <td>128.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>132.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>251.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales  total-spend\n",
       "0  230.1   37.8       69.2   22.1        337.1\n",
       "1   44.5   39.3       45.1   10.4        128.9\n",
       "2   17.2   45.9       69.3    9.3        132.4\n",
       "3  151.5   41.3       58.5   18.5        251.3\n",
       "4  180.8   10.8       58.4   12.9        250.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'.\\Recursos\\08-Linear-Regression-Models\\Advertising.csv')\n",
    "df['total-spend'] = df.iloc[:,:-1].sum(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['total-spend'].head(25).values\n",
    "y = df['sales'].head(25).values\n",
    "y_ = 0.005 * X + y.mean()\n",
    "\n",
    "residual = y - y_\n",
    "error = residual**2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "ax[0].scatter(X, y, edgecolor='black', label=r'$y$')\n",
    "\n",
    "ax[1].scatter(X, y, edgecolor='black', label=r'$y$')\n",
    "ax[1].plot(X, y_, color='red', linewidth=1, label=r'$\\hat{y}$')\n",
    "ax[1].vlines(X, ymin=y_, ymax=y, linestyle='--', color='orange', label=r'$y - \\hat{y}$')\n",
    "\n",
    "plt.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contudo, como estimar os valores dos coeficientes $\\beta_0$ e $\\beta_1$ da reta que melhor descreve o problema?\n",
    "\n",
    "A partir da relação estabelecida e da análise do gráfico, temos\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i $$\n",
    "\n",
    "$$ \\hat{y}_i = \\beta_0 + \\beta_1 \\cdot x_i $$\n",
    "\n",
    "$$ y_i = \\hat{y}_i + \\epsilon_i $$\n",
    "\n",
    "$$ y_i - \\hat{y}_i = \\epsilon_i $$\n",
    "\n",
    "$$ \\sum_{i=1}^n y_i - \\hat{y}_i = \\sum_{i=1}^n \\epsilon_i $$\n",
    "\n",
    "Portanto, a reta que melhor descreve o problema é aquele que minimiza a soma das diferenças entre os valores calculados e reais, ou seja, minimiza a soma dos erros $\\epsilon_i$. Os valores de $\\epsilon_i$ podem ser negativos e positivos. Dessa forma, ao invés da soma absoluta, é minimizado o erro médio quadrático (MSE):\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min = y.min() - 5\n",
    "y_max = y.max() + 5\n",
    "\n",
    "a_min = (y_min - y_max) / (X.max() - X.min())\n",
    "a_max = - a_min\n",
    "\n",
    "m = 25\n",
    "a = np.linspace(a_min, a_max, m)\n",
    "b = np.linspace(y_min, y_max, m)\n",
    "\n",
    "A, B = np.meshgrid(a,b)\n",
    "\n",
    "error_surface = ((y - (A.reshape(-1,1) * X.T + B.reshape(-1,1)))**2).mean(axis=1).reshape(m, m)\n",
    "\n",
    "y_min, y_max = y.min(), y.max()\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "# plt.subplots_adjust(bottom=0.25)\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "ax1.scatter(X, y, edgecolor='black')\n",
    "ax1_line, = ax1.plot(X, y_, color='red', linewidth=1)\n",
    "ax1_vlines = ax1.vlines(X, ymin=y_, ymax=y, linestyle='--', color='orange')\n",
    "ax1.set_ylim(y.min() - 5, y.max() + 5)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "\n",
    "ax2.contourf(A, B, error_surface, 100, cmap='coolwarm')\n",
    "ax2_scatter = ax2.scatter([], [], ec='black', s=80, color='yellow')\n",
    "ax2.set_xlabel(r'$\\beta_1$')\n",
    "ax2.set_ylabel(r'$\\beta_0$')\n",
    "\n",
    "ax3.plot_surface(A, B, error_surface, cmap='coolwarm', zorder=0, linewidth=0)\n",
    "ax3_line, = ax3.plot([], [], [], marker='o', color='yellow', zorder=2)\n",
    "ax3.set_xlabel(r'$\\beta_1$')\n",
    "ax3.set_ylabel(r'$\\beta_0$')\n",
    "ax3.set_zlabel('MSE')\n",
    "\n",
    "def update(a, b):\n",
    "\n",
    "    y_ = a * X + b\n",
    "    error = ((y - y_)**2).mean()\n",
    "    \n",
    "    ax1_line.set_ydata([y_])\n",
    "    ax1_vlines.set_segments([[[xi, y_i], [xi, yi]] for xi, yi, y_i in zip(X, y, y_)])\n",
    "\n",
    "    ax2_scatter.set_offsets([a, b])\n",
    "\n",
    "    ax3_line.set_data([a], [b])\n",
    "    ax3_line.set_3d_properties(error)\n",
    "\n",
    "interact(\n",
    "    update,\n",
    "    a=widgets.FloatSlider(value=a[m//2], min=a[0], max=a[-1], step=1e-4, readout=True, readout_format='.4f'),\n",
    "    b=widgets.FloatSlider(value=b[m//2], min=b[0], max=b[-1], step=1e-2, readout=True, readout_format='.2f')\n",
    ")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo o erro médio quadrático em função dos parâmetros, temos\n",
    "\n",
    "$$ J(\\beta_0, \\beta_1) = \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{m} [y_i - (\\beta_0 + \\beta_1 \\cdot x_i)]^2 $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{m} y_i^2 - 2 y_i (\\beta_0 + \\beta_1 \\cdot x_i) + (\\beta_0 + \\beta_1 \\cdot x_i)^2 $$\n",
    "\n",
    "$$ = \\sum_{i=1}^{m} y_i^2 -2 y_i \\beta_0 - 2 y_i \\beta_1 x + \\beta_0^2 + 2 \\beta_0 \\beta_1 x + \\beta_1^2 x^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar o erro $L$ mínimo, calcula-se a derivada do erro com relação aos parâmetros $\\beta_0$ e $\\beta_1$ e as iguala a zero:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta_0} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 = 0 = \\sum_{i=1}^{m} -2 y_i + 2 \\beta_0 + 2 \\beta_1 x_i $$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 = 0 = \\sum_{i=1}^{m} -2 y_i x_i + 2 \\beta_0 x_i + 2 \\beta_1 x_i^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomando a primeira equação e isolando o parâmetro $\\beta_0$, temos\n",
    "\n",
    "$$ \\sum_{i=1}^{m} y_i = \\sum_{i=1}^{m} \\beta_0 + \\sum_{i=1}^{m} \\beta_1 x_i $$\n",
    "\n",
    "$$ \\sum_{i=1}^{m} y_i = m \\beta_0 + \\beta_1 \\sum_{i=1}^{m} x_i $$\n",
    "\n",
    "$$ \\beta_0 = \\frac{1}{m} \\left(\\sum_{i=1}^{m} y_i - \\beta_1 \\sum_{i=1}^{m} x_i \\right) $$\n",
    "\n",
    "$$ \\beta_0 = \\overline{y} - \\beta_1 \\overline{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomando a segunda equação, isolando o parâmetro $\\beta_1$ e substituindo o valor de $\\beta_0$ encontrado anteriormente, temos\n",
    "\n",
    "$$ \\sum_{i=1}^{m} y_i x_i = \\sum_{i=1}^{m} \\beta_0 x_i + \\sum_{i=1}^{m} \\beta_1 x_i^2 $$\n",
    "\n",
    "$$ \\sum_{i=1}^{m} y_i x_i = (\\overline{y} - \\beta_1 \\overline{x}) \\sum_{i=1}^{m} x_i + \\beta_1 \\sum_{i=1}^{m} x_i^2 $$\n",
    "\n",
    "$$ \\sum_{i=1}^{m} y_i x_i = \\overline{y} \\sum_{i=1}^{m} x_i - \\beta_1 \\overline{x} \\sum_{i=1}^{m} x_i + \\beta_1 \\sum_{i=1}^{m} x_i^2 $$\n",
    "\n",
    "$$ \\sum_{i=1}^{m} x_i y_i = \\overline{y} \\sum_{i=1}^{m} x_i + \\beta_1 \\left(\\sum_{i=1}^{m} x_i^2 - \\overline{x} \\sum_{i=1}^{m} x_i \\right) $$\n",
    "\n",
    "$$ \\beta_1 = \\frac{\\sum_{i=1}^{m} x_i y_i - \\overline{y} \\sum_{i=1}^{m} x_i}{\\sum_{i=1}^{m} x_i^2 - \\overline{x} \\sum_{i=1}^{m} x_i} $$\n",
    "\n",
    "$$ \\beta_1 = \\frac{\\sum_{i=1}^{m} y_i x_i - m \\overline{y} \\cdot \\overline{x}}{\\sum_{i=1}^{m} x_i^2 - m \\overline{x}^2} $$\n",
    "\n",
    "$$ \\beta_1 = \\frac{\\overline{y x} - \\overline{y} \\cdot \\overline{x}}{\\overline{x^2} - \\overline{x}^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = ((X * y).mean() - X.mean() * y.mean()) / ((X**2).mean() - X.mean()**2)\n",
    "b0 = y.mean() - a0 * X.mean()\n",
    "\n",
    "print(f'beta0 = {round(b0, 3)} | beta1 = {round(a0, 3)}')\n",
    "\n",
    "y_ = a0 * X + b0\n",
    "error = ((y - y_)**2).mean()\n",
    "\n",
    "error_surface = ((y - (A.reshape(-1,1) * X.T + B.reshape(-1,1)))**2).mean(axis=1).reshape(m, m)\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133, projection='3d') \n",
    "\n",
    "ax1.scatter(X, y, edgecolor='black')\n",
    "ax1.plot(X, y_, color='red', linewidth=1)\n",
    "ax1.vlines(X, ymin=y_, ymax=y, linestyle='--', color='orange')\n",
    "ax1.set_xlabel(r'$x$')\n",
    "ax1.set_ylabel(r'$y$')\n",
    "\n",
    "ax2.contourf(A, B, error_surface, 100, cmap='coolwarm')\n",
    "ax2.scatter(a0, b0, ec='black', s=80, color='yellow')\n",
    "ax2.set_xlabel(r'$\\beta_1$')\n",
    "ax2.set_ylabel(r'$\\beta_0$')\n",
    "\n",
    "ax3.plot_surface(A, B, error_surface, cmap='coolwarm', zorder = 0, linewidth=0)\n",
    "ax3.plot(a0, b0, error, marker='o', color='yellow', zorder = 2)\n",
    "ax3.set_xlabel(r'$\\beta_1$')\n",
    "ax3.set_ylabel(r'$\\beta_0$')\n",
    "ax3.set_zlabel(r'MSE')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para problemas com múltiplas variáveis independentes, encontrar a solução analítica para o problema de mínimos quadrados se torna praticamente inviável. Para contornar este problema, são utilizados dois métodos: $\\textbf{Gradiente Descendente}$ e $\\textbf{matriz pseudo-inversa}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar a implementação computacional, é comum a utilização da notação matricial, assim\n",
    "\n",
    "$$ \\hat{Y} = X \\beta $$\n",
    "\n",
    "onde $\\hat{Y}$ é o vetor correspondente às respostas calculadas, $\\beta$ o vetor correspondente aos coeficientes e $X$ a matriz correspondente às variáveis independentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{Y} = \\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\hat{y}_2 \\\\\n",
    "\\hat{y}_3 \\\\\n",
    "\\dots \\\\\n",
    "\\hat{y}_m\n",
    "\\end{bmatrix}, $$\n",
    "\n",
    "$$ \\beta = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\dots \\\\\n",
    "\\beta_n\n",
    "\\end{bmatrix}, $$\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 & x_{1,2} & x_{1,3} & \\dots & x_{1,n} \\\\\n",
    "1 & x_{2,2} & x_{2,3} & \\dots & x_{2,n} \\\\\n",
    "1 & x_{3,2} & x_{3,3} & \\dots & x_{3,n} \\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "1 & x_{m,2} & x_{m,3} & \\dots & x_{m,n}\n",
    "\\end{bmatrix}. $$\n",
    "\n",
    "$$ \\hat{Y} = X \\beta = \\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\hat{y}_2 \\\\\n",
    "\\hat{y}_3 \\\\\n",
    "\\dots \\\\\n",
    "\\hat{y}_m\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & x_{1,2} & x_{1,3} & \\dots & x_{1,n} \\\\\n",
    "1 & x_{2,2} & x_{2,3} & \\dots & x_{2,n} \\\\\n",
    "1 & x_{3,2} & x_{3,3} & \\dots & x_{3,n} \\\\\n",
    "\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
    "1 & x_{m,2} & x_{m,3} & \\dots & x_{m,n}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\dots \\\\\n",
    "\\beta_n\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solução Matricial (Pseudo-Inversa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos anteriormente que \n",
    "\n",
    "$$ \\hat{Y} = X \\beta$$\n",
    "\n",
    "Portanto,\n",
    "\n",
    "$$ e = Y - \\hat{Y} $$\n",
    "$$ = Y - X \\beta $$\n",
    "\n",
    "$$ \\text{MSE} = J(\\beta) = e^T e $$\n",
    "$$ = (Y - \\hat{Y})^T (Y - \\hat{Y}) $$\n",
    "$$ = (Y - X \\beta)^T (Y - X \\beta) $$\n",
    "$$ = \\left(Y^T - (X \\beta)^T \\right) (Y - X \\beta) $$\n",
    "$$ = (Y^T - \\beta^T X^T) (Y - X \\beta) $$\n",
    "$$ = Y^T Y - Y^T X \\beta - \\beta^T X^T Y + \\beta^T X^T X \\beta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para minimizar o $\\text{J}(\\beta)$, tomamos a derivada em relação a $\\beta$ e igualamos a zero:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\text{MSE}}{\\partial \\beta} = 0 = 0 - X^T Y - X^T Y + 2 X^T X \\beta$$\n",
    "$$ = X^T X \\beta - X^T Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, é obtido o sistema de equações normais:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X^T X \\beta = X^T Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter a solução da equação anterior, devemos multiplicar ambos os lados da igualdade pela inversa de $X^T X$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ (X^T X)^{-1} X^T X \\beta = (X^T X)^{-1} X^T Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como $(X^T X)^{-1} (X^T X) = I$ e $I \\beta = \\beta$, temos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\beta = (X^T X)^{-1} X^T Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([np.ones(len(X)).reshape(-1,1), X.reshape(-1,1)], axis=1)\n",
    "\n",
    "beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(rf'beta0 = {round(b0, 3)} | beta1 = {round(a0, 3)}')\n",
    "\n",
    "y_ = X.dot(beta)\n",
    "error = ((y - y_)**2).mean()\n",
    "\n",
    "error_surface = ((y - (A.reshape(-1,1) * X[:,1].T + B.reshape(-1,1)))**2).mean(axis=1).reshape(m, m)\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133, projection='3d') \n",
    "\n",
    "ax1.scatter(X[:,1], y, edgecolor='black')\n",
    "ax1.plot(X[:,1], y_, color='red', linewidth=1)\n",
    "ax1.vlines(X[:,1], ymin=y_, ymax=y, linestyle='--', color='orange')\n",
    "ax1.set_xlabel(r'$x$')\n",
    "ax1.set_ylabel(r'$y$')\n",
    "\n",
    "ax2.contourf(A, B, error_surface, 100, cmap='coolwarm')\n",
    "ax2.scatter(a0, b0, ec='black', s=80, color='yellow')\n",
    "ax2.set_xlabel(r'$\\beta_1$')\n",
    "ax2.set_ylabel(r'$\\beta_0$')\n",
    "\n",
    "ax3.plot_surface(A, B, error_surface, cmap='coolwarm', zorder = 0, linewidth=0)\n",
    "ax3.plot(a0, b0, error, marker='o', color='yellow', zorder = 2)\n",
    "ax3.set_xlabel(r'$\\beta_1$')\n",
    "ax3.set_ylabel(r'$\\beta_0$')\n",
    "ax3.set_zlabel(r'MSE')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradiente Descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalizando o cálulo das derivadas, temos\n",
    "\n",
    "$$ \\frac{\\partial J(\\beta)}{\\partial \\beta_k} = \\frac{2}{m} \\sum_{i=1}^m \\left(y_i - \\sum_{j=0}^n \\beta_j x_{i,j} \\right) (-x_{i,k}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar o gradiente $\\nabla$ para expressar a derivada da função custo com relação a todos os coeficientes $\\beta$, de acordo com a expressão $\\nabla_{\\beta}J$. Dessa forma,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\nabla_{\\beta}J = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\beta_0} &\n",
    "\\frac{\\partial J}{\\partial \\beta_1} &\n",
    "\\dots &\n",
    "\\frac{\\partial J}{\\partial \\beta_n}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ \\nabla_{\\beta}J = \\begin{bmatrix}\n",
    "\\frac{1}{m} \\sum_{i=1}^m \\left(y_i - \\sum_{j=0}^n \\beta_j x_{i,j} \\right) (-x_{i,0}) \\\\\n",
    "\\frac{1}{m} \\sum_{i=1}^m \\left(y_i - \\sum_{j=0}^n \\beta_j x_{i,j} \\right) (-x_{i,1}) \\\\\n",
    "\\dots \\\\\n",
    "\\frac{1}{m} \\sum_{i=1}^m \\left(y_i - \\sum_{j=0}^n \\beta_j x_{i,j} \\right) (-x_{i,n})\n",
    "\\end{bmatrix}^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, é possível atualizar os parâmetros de acordo com o gradiente na direção de mínimo\n",
    "\n",
    "$$ \\beta_{t+1} = \\beta_t - \\alpha \\cdot \\nabla_{\\beta}J $$\n",
    "\n",
    "onde $\\beta_{t+1}$ é o parâmetro atualizado, $\\beta_t$ o parâmetro atual, $\\alpha$ uma constante chamada $\\textit{learning rate}$, que controla a intensidade da atualização, e $\\nabla_{\\beta}J$ o vetor gradiente da função custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:25, -1].values.reshape(-1,1)\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "X = np.concatenate([np.ones(len(X)).reshape(-1,1), X], axis=1)\n",
    "\n",
    "y = df.iloc[:25, -2].values + 40\n",
    "\n",
    "beta = np.asarray([0., 0.])\n",
    "\n",
    "history = {\n",
    "    'loss' : [],\n",
    "    'beta' : [],\n",
    "    'y_pred' : []\n",
    "}\n",
    "\n",
    "m = len(y)\n",
    "lr = 1e-01\n",
    "epochs = 50\n",
    "\n",
    "for _ in range(epochs):\n",
    "    y_pred = X.dot(beta)\n",
    "    error = y_pred - y\n",
    "    mse = error.T.dot(error) / m\n",
    "    gradient = -2 * X.T.dot(error) / m\n",
    "    beta += lr * gradient\n",
    "\n",
    "    history['loss'].append(mse)\n",
    "    history['beta'].append(beta.round(2))\n",
    "    history['y_pred'].append(y_pred)\n",
    "\n",
    "beta0 = np.linspace(0, 100, 25)\n",
    "beta1 = np.linspace(-100, 100, 25)\n",
    "beta0, beta1 = np.meshgrid(beta0, beta1)\n",
    "beta_surface = np.concatenate([beta0.reshape(-1,1), beta1.reshape(-1,1)], axis=1)\n",
    "\n",
    "error_surface = y.reshape(-1,1) - X.dot(beta_surface.T)\n",
    "mse_surface = np.asarray([line.T.dot(line) for line in error_surface.T]).reshape(25,25) / m\n",
    "\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(235, projection='3d')\n",
    "\n",
    "ax1.plot(history['loss'], color='black', label='Custo (não normalizado)', zorder=0)\n",
    "ax1_scatter = ax1.scatter([], [], c='yellow', ec='black', zorder=1)\n",
    "ax1.set_title('Curva de Aprendizado')\n",
    "ax1.set_xlabel('Época')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.grid(False)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(X[:,1], y, color='orange', ec='black', label=r'Observado ($y$)', zorder=1)\n",
    "ax2_line, = ax2.plot(X[:,1], X.dot(beta), color='black', linewidth=1, label=r'Calculado ($X\\beta$)', zorder=0)\n",
    "ax2_annot = ax2.annotate(text='', xy=(-3,80))\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_xlim(-5,5)\n",
    "ax2.grid(False)\n",
    "ax2.legend()\n",
    "\n",
    "ax3.plot_surface(beta0, beta1, mse_surface, cmap='coolwarm', lw=0, zorder=0)\n",
    "ax3_line, = ax3.plot([], [], [], marker='o', c='yellow', zorder=2)\n",
    "\n",
    "def update(epoch):\n",
    "    ax1_scatter.set_offsets([epoch, history['loss'][epoch]])\n",
    "\n",
    "    ax2_line.set_ydata(history['y_pred'][epoch])\n",
    "    ax2_annot.set_text(rf'$\\beta$ = {history[\"beta\"][epoch]}')\n",
    "    ax2.set_title(f'Distribuição e Modelo Ajustado \\nÉpoca: {epoch}')\n",
    "\n",
    "    ax3_line.set_data([history['beta'][epoch][0]], [history['beta'][epoch][1]])\n",
    "    ax3_line.set_3d_properties([history['loss'][epoch]])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "interact(update, epoch=widgets.IntSlider(value=0, min=0, max=epochs-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
